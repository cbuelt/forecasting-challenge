{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dcc675b-a528-47d8-817d-66cd81b07b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler, LabelEncoder\n",
    "from tensorflow_addons.losses import pinball_loss\n",
    "from sklearn.metrics import mean_pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a7bd45-3634-4ef0-9ea6-38c2b9c97c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "horizons = [36, 48 ,60, 72, 84]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4510d11-04a6-4c31-a2ec-0b842336dafc",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33903f39-2172-42bf-8827-7ec99afc0f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>init_tm</th>\n",
       "      <th>met_var</th>\n",
       "      <th>location</th>\n",
       "      <th>fcst_hour</th>\n",
       "      <th>obs_tm</th>\n",
       "      <th>obs</th>\n",
       "      <th>ens_1</th>\n",
       "      <th>ens_2</th>\n",
       "      <th>ens_3</th>\n",
       "      <th>ens_4</th>\n",
       "      <th>...</th>\n",
       "      <th>ens_33</th>\n",
       "      <th>ens_34</th>\n",
       "      <th>ens_35</th>\n",
       "      <th>ens_36</th>\n",
       "      <th>ens_37</th>\n",
       "      <th>ens_38</th>\n",
       "      <th>ens_39</th>\n",
       "      <th>ens_40</th>\n",
       "      <th>ens_mean</th>\n",
       "      <th>ens_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-13 00:00:00+00:00</td>\n",
       "      <td>t_2m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-12-13 00:00:00+00:00</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.34500</td>\n",
       "      <td>0.148036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-13 00:00:00+00:00</td>\n",
       "      <td>t_2m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-12-13 01:00:00+00:00</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.30125</td>\n",
       "      <td>0.162580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-13 00:00:00+00:00</td>\n",
       "      <td>t_2m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-12-13 02:00:00+00:00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.32525</td>\n",
       "      <td>0.171626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-13 00:00:00+00:00</td>\n",
       "      <td>t_2m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-12-13 03:00:00+00:00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.31700</td>\n",
       "      <td>0.161878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-13 00:00:00+00:00</td>\n",
       "      <td>t_2m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-12-13 04:00:00+00:00</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.29275</td>\n",
       "      <td>0.150544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    init_tm met_var location  fcst_hour  \\\n",
       "0 2018-12-13 00:00:00+00:00    t_2m   Berlin        0.0   \n",
       "1 2018-12-13 00:00:00+00:00    t_2m   Berlin        1.0   \n",
       "2 2018-12-13 00:00:00+00:00    t_2m   Berlin        2.0   \n",
       "3 2018-12-13 00:00:00+00:00    t_2m   Berlin        3.0   \n",
       "4 2018-12-13 00:00:00+00:00    t_2m   Berlin        4.0   \n",
       "\n",
       "                     obs_tm  obs  ens_1  ens_2  ens_3  ens_4  ...  ens_33  \\\n",
       "0 2018-12-13 00:00:00+00:00  2.3   0.05   0.03   0.32   0.61  ...    0.30   \n",
       "1 2018-12-13 01:00:00+00:00  2.3   0.06   0.00   0.34   0.51  ...    0.13   \n",
       "2 2018-12-13 02:00:00+00:00  2.2   0.10   0.11   0.40   0.45  ...    0.08   \n",
       "3 2018-12-13 03:00:00+00:00  2.2   0.12   0.32   0.48   0.42  ...    0.05   \n",
       "4 2018-12-13 04:00:00+00:00  2.1   0.11   0.57   0.54   0.35  ...    0.03   \n",
       "\n",
       "   ens_34  ens_35  ens_36  ens_37  ens_38  ens_39  ens_40  ens_mean   ens_var  \n",
       "0   -0.03    0.44    0.42    0.05    0.47    0.51    0.74   0.34500  0.148036  \n",
       "1    0.05    0.39    0.38    0.04    0.38    0.33    0.60   0.30125  0.162580  \n",
       "2    0.15    0.57    0.37    0.08    0.42    0.31    0.57   0.32525  0.171626  \n",
       "3    0.19    0.68    0.32    0.08    0.28    0.35    0.60   0.31700  0.161878  \n",
       "4    0.17    0.76    0.30    0.10    0.11    0.34    0.56   0.29275  0.150544  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_t_2m.feather\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3daf8141-c74c-4a94-8c48-d3a39cf41cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    data_prep = data.dropna().copy()\n",
    "    data_prep.drop([\"init_tm\", \"met_var\", \"location\", \"ens_mean\", \"ens_var\", \"obs_tm\"], axis = 1, inplace = True)\n",
    "    return data_prep.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6259e9f7-b645-41d5-a3db-7f6f3970a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = prepare_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3556da-7ec8-4619-93f2-a911db38f0db",
   "metadata": {},
   "source": [
    "## Train, val, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6c0048-357c-4145-9a03-f78d32d81a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(data_np, test_size = 0.2)\n",
    "train, val = train_test_split(train_val, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dca02c-d2f6-4e83-b68b-a0f78e15466d",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19dee4e5-8eba-4f60-a4ad-75d930725ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, feature_scaler = None, target_scaler = None, learn = False):\n",
    "    if learn == True:\n",
    "        feature_scaler = StandardScaler()\n",
    "        target_scaler = StandardScaler()\n",
    "        #Learn target scaling\n",
    "        target_scaled = target_scaler.fit_transform(data[:,1].reshape(-1,1))\n",
    "        #Learn feature scaling\n",
    "        feature_scaled = feature_scaler.fit_transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data, feature_scaler, target_scaler\n",
    "    \n",
    "    else:\n",
    "        #Scale target\n",
    "        target_scaled = target_scaler.transform(data[:,1].reshape(-1,1))\n",
    "        #Scale features\n",
    "        feature_scaled = feature_scaler.transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bab981b-fa00-4102-9f15-a7b9b6baf764",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, feature_scaler, target_scaler = normalize(train, learn = True)\n",
    "train_val = normalize(train_val, feature_scaler, target_scaler)\n",
    "test = normalize(test, feature_scaler, target_scaler)\n",
    "val = normalize(val, feature_scaler, target_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91ce055-b236-4060-926a-36fa33dea304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(input_data, predict = False):\n",
    "    #Extract forecast embedding\n",
    "    horizon_emb = input_data[:,0]\n",
    "    \n",
    "    if predict == False:        \n",
    "        #Extract features\n",
    "        features = input_data[:,2:]\n",
    "        # Extract target\n",
    "        target = np.expand_dims(input_data[:,1],1)\n",
    "        return [features, horizon_emb], target\n",
    "    else:\n",
    "        #Extract features\n",
    "        features = input_data[:,1:]\n",
    "        return [features, horizon_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55bd7ffb-df12-420f-80f7-cab75ab53795",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target = convert_format(train)\n",
    "val_data, val_target = convert_format(val)\n",
    "test_data, test_target = convert_format(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe6315-fa3a-4000-a2de-7c7714564f6e",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c401b24-2bb3-4032-8387-68d8759625eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "650ec674-e165-43a9-b763-fe6b866ac6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model(tf.keras.Model):    \n",
    "    def __init__(self, n_embeddings = 121):\n",
    "        super(base_model, self).__init__()\n",
    "        #Embedding layers\n",
    "        self.embedding = Embedding(input_dim = n_embeddings, output_dim = 4)\n",
    "        #Create Dense layers\n",
    "        self.hidden = Dense(25, activation = \"relu\")\n",
    "        self.out = Dense(1, activation = \"linear\")\n",
    "\n",
    "    def call(self, input_data):\n",
    "        #Extract data\n",
    "        features, horizon_emb = input_data\n",
    "        #Calculate embedding\n",
    "        emb = self.embedding(horizon_emb)\n",
    "        emb = tf.squeeze(emb, axis = 1)\n",
    "        conc = Concatenate(axis = 1)([features, emb])\n",
    "        #Calculate output\n",
    "        output = self.hidden(conc)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9260e5d-229c-41e5-9411-64bb7c727707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, quantile, train_data, train_target, validation_data, batch_size, epochs, learning_rate):\n",
    "    #Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    #Early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5, min_delta = 1e-5)\n",
    "    #Compile model\n",
    "    model.compile(optimizer = optimizer, loss = lambda true,pred: pinball_loss(true, pred, tau = quantile))\n",
    "    model.fit(x = train_data, y = train_target, validation_data = validation_data, epochs = epochs, batch_size = batch_size, callbacks = [callback], shuffle = True, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60d1f33-79ac-4881-8b89-c7afb299c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_models(train_data, train_target, validation_data, batch_size = BATCH_SIZE, epochs = EPOCHS, learning_rate = learning_rate, quantiles = quantiles):\n",
    "    models = []\n",
    "    for quantile in quantiles:\n",
    "        model = base_model()\n",
    "        train_model(model, quantile, train_data, train_target, validation_data, batch_size, epochs, learning_rate)\n",
    "        print(\"Training finished for quantile: {}\".format(quantile))\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4835ea46-5637-496d-97a5-e22b4be0c6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for quantile: 0.025\n",
      "Training finished for quantile: 0.25\n",
      "Training finished for quantile: 0.5\n",
      "Training finished for quantile: 0.75\n",
      "Training finished for quantile: 0.975\n"
     ]
    }
   ],
   "source": [
    "trained_models = create_all_models(train_data, train_target, (val_data, val_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77040432-055d-4329-b3c5-832009a895bc",
   "metadata": {},
   "source": [
    "# Predict test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c449a-9d86-4c65-bf50-70fe9e426cd7",
   "metadata": {},
   "source": [
    "## Evaluate pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5b603f-9ae8-4e1c-ad60-3fe8cb878c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for cnt,_ in enumerate(quantiles):\n",
    "    pred = trained_models[cnt].predict(test_data)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de39b681-c77e-4eda-b524-ff7634070be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinball loss for quantile 0.025 : \t 0.012062061142408834\n",
      "Pinball loss for quantile 0.25 : \t 0.055619141629077586\n",
      "Pinball loss for quantile 0.5 : \t 0.06365643585367962\n",
      "Pinball loss for quantile 0.75 : \t 0.0500263623120046\n",
      "Pinball loss for quantile 0.975 : \t 0.009865157278391753\n"
     ]
    }
   ],
   "source": [
    "for cnt,quantile in enumerate(quantiles):\n",
    "    loss = mean_pinball_loss(test_target.reshape(-1), predictions[cnt].reshape(-1), alpha = quantile)\n",
    "    print(\"Pinball loss for quantile {} : \\t {}\".format(quantile,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06d9e6-501e-4dc9-9774-e2eeb9f3df0d",
   "metadata": {},
   "source": [
    "## Evaluate pinball loss for naive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c292058-fa01-498e-947c-513092bb5be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinball loss for quantile 0.025 : \t 0.01689490236482793\n",
      "Pinball loss for quantile 0.25 : \t 0.05369620855970884\n",
      "Pinball loss for quantile 0.5 : \t 0.0642630281986549\n",
      "Pinball loss for quantile 0.75 : \t 0.053349433176900135\n",
      "Pinball loss for quantile 0.975 : \t 0.01746772324124291\n"
     ]
    }
   ],
   "source": [
    "naive_pred = np.quantile(test_data[0], quantiles, axis = 1)\n",
    "for cnt,quantile in enumerate(quantiles):\n",
    "    loss = mean_pinball_loss(np.squeeze(test_target), naive_pred[cnt], alpha = quantile)\n",
    "    print(\"Pinball loss for quantile {} : \\t {}\".format(quantile,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1cbc6-a34a-4719-bd4d-f164a8ae4029",
   "metadata": {},
   "source": [
    "## Check plausability of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdbf0f5a-5eed-4f66-b62a-c54cc195a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile met for quantile = 0.025: \t 4.31 %\n",
      "Quantile met for quantile = 0.25: \t 36.36 %\n",
      "Quantile met for quantile = 0.5: \t 51.99 %\n",
      "Quantile met for quantile = 0.75: \t 72.11 %\n",
      "Quantile met for quantile = 0.975: \t 96.93 %\n"
     ]
    }
   ],
   "source": [
    "for cnt,pred in enumerate(predictions):\n",
    "    q_smaller = (pred > test_target).sum()\n",
    "    emp_quant = q_smaller / pred.size\n",
    "    print(\"Quantile met for quantile = {}: \\t {} %\".format(quantiles[cnt], np.round(emp_quant,4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa53c3-7838-4d56-b1df-d3bc626bdd52",
   "metadata": {},
   "source": [
    "# Predict new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e557515-41c9-4fff-8e2b-e9d3b44837de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train on complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b9b4ac5-2dd0-44ce-89a4-4e7f3362d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target = convert_format(train_val)\n",
    "val_data, val_target = convert_format(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39bec11b-7fba-4209-923d-85d082a1ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for quantile: 0.025\n",
      "Training finished for quantile: 0.25\n",
      "Training finished for quantile: 0.5\n",
      "Training finished for quantile: 0.75\n",
      "Training finished for quantile: 0.975\n"
     ]
    }
   ],
   "source": [
    "trained_models = create_all_models(train_data, train_target, (val_data, val_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62998b-cc46-461b-b433-a5a42a637265",
   "metadata": {},
   "source": [
    "## Predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "766e47da-37bc-4465-b36a-1990b4820fb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/berlin_data/icon_data/icon-eu-eps_2021112400_t_2m_Berlin.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25908/4186719386.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcurrent_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/berlin_data/icon_data/icon-eu-eps_{}00_t_2m_Berlin.txt\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_date\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Normalize and get horizons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/berlin_data/icon_data/icon-eu-eps_2021112400_t_2m_Berlin.txt'"
     ]
    }
   ],
   "source": [
    "#Set current date\n",
    "current_date = (date.today()).strftime(\"%Y%m%d\")\n",
    "path = \"data/berlin_data/icon_data/icon-eu-eps_{}00_t_2m_Berlin.txt\".format(current_date)\n",
    "new_data = pd.read_csv(path.format(current_date.replace(\"-\",\"\")), skiprows = 3, sep = \"|\").dropna(axis = 1)\n",
    "new_data.columns = new_data.columns.str.replace(\" \", \"\")\n",
    "# Normalize and get horizons\n",
    "new_data = new_data[new_data[\"fcst_hour\"].isin(horizons)].to_numpy()\n",
    "new_data[:,1:] = feature_scaler.transform(new_data[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d47376d-15f3-41d4-bb62-7c6b26fd78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = convert_format(new_data, predict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81cc69c7-8540-47c6-be2f-cfd97b244c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataframe\n",
    "final_prediction = pd.DataFrame(columns = [\"forecast_date\",\"target\",\"horizon\",\"q0.025\",\"q0.25\",\"q0.5\",\"q0.75\",\"q0.975\"], index = np.arange(0,5))\n",
    "final_prediction[\"forecast_date\"] = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "final_prediction[\"horizon\"] = [\"{} hour\".format(x) for x in horizons]\n",
    "final_prediction[\"target\"] = \"temperature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa59a62f-fb9b-45e9-aa3e-cf8fbb366c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 408 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000015716D869D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 409 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000015717E51820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "for cnt, quantile in enumerate(quantiles):\n",
    "    #Get prediction\n",
    "    prediction = trained_models[cnt].predict(pred_data)\n",
    "    #Retransform\n",
    "    final_pred = target_scaler.inverse_transform(prediction)\n",
    "    final_prediction.loc[:,\"q{}\".format(quantile)] = final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b85780f-828b-4b27-800f-a70956cf7b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast_date</th>\n",
       "      <th>target</th>\n",
       "      <th>horizon</th>\n",
       "      <th>q0.025</th>\n",
       "      <th>q0.25</th>\n",
       "      <th>q0.5</th>\n",
       "      <th>q0.75</th>\n",
       "      <th>q0.975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-17</td>\n",
       "      <td>temperature</td>\n",
       "      <td>36 hour</td>\n",
       "      <td>6.916572</td>\n",
       "      <td>8.300205</td>\n",
       "      <td>9.252678</td>\n",
       "      <td>9.738008</td>\n",
       "      <td>11.521322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-17</td>\n",
       "      <td>temperature</td>\n",
       "      <td>48 hour</td>\n",
       "      <td>7.463832</td>\n",
       "      <td>9.023012</td>\n",
       "      <td>9.633258</td>\n",
       "      <td>10.287542</td>\n",
       "      <td>11.583526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-17</td>\n",
       "      <td>temperature</td>\n",
       "      <td>60 hour</td>\n",
       "      <td>9.062678</td>\n",
       "      <td>11.024169</td>\n",
       "      <td>11.886627</td>\n",
       "      <td>13.045918</td>\n",
       "      <td>15.409112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-17</td>\n",
       "      <td>temperature</td>\n",
       "      <td>72 hour</td>\n",
       "      <td>8.407082</td>\n",
       "      <td>10.224401</td>\n",
       "      <td>10.825706</td>\n",
       "      <td>12.006133</td>\n",
       "      <td>13.478199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-17</td>\n",
       "      <td>temperature</td>\n",
       "      <td>84 hour</td>\n",
       "      <td>8.455604</td>\n",
       "      <td>10.755701</td>\n",
       "      <td>11.453940</td>\n",
       "      <td>12.665792</td>\n",
       "      <td>15.562379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  forecast_date       target  horizon    q0.025      q0.25       q0.5  \\\n",
       "0    2021-11-17  temperature  36 hour  6.916572   8.300205   9.252678   \n",
       "1    2021-11-17  temperature  48 hour  7.463832   9.023012   9.633258   \n",
       "2    2021-11-17  temperature  60 hour  9.062678  11.024169  11.886627   \n",
       "3    2021-11-17  temperature  72 hour  8.407082  10.224401  10.825706   \n",
       "4    2021-11-17  temperature  84 hour  8.455604  10.755701  11.453940   \n",
       "\n",
       "       q0.75     q0.975  \n",
       "0   9.738008  11.521322  \n",
       "1  10.287542  11.583526  \n",
       "2  13.045918  15.409112  \n",
       "3  12.006133  13.478199  \n",
       "4  12.665792  15.562379  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95977220-fde7-4cd4-a1e9-47330f8d52bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36     8.66\n",
       "48     9.13\n",
       "52    11.89\n",
       "56    10.64\n",
       "58    11.30\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.read_csv(path.format(current_date.replace(\"-\",\"\")), skiprows = 3, sep = \"|\").dropna(axis = 1)\n",
    "new_data.columns = new_data.columns.str.replace(\" \", \"\")\n",
    "new_data[new_data[\"fcst_hour\"].isin(horizons)].quantile(0.5, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d25019c0-38cc-4bae-9aff-24414fda88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction.to_pickle(\"../evaluation/predictions/single/{}_{}\".format(\"temperature\", date.today().strftime(\"%Y-%m-%d\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd0d39-9e8d-446a-9ac6-c329c3bf9d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
