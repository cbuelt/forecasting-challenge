{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2684caa-a520-4b33-909a-fab7d08346a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler, LabelEncoder\n",
    "from tensorflow_addons.losses import pinball_loss\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from scipy import stats\n",
    "import math\n",
    "import optuna as opt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tensorflow_addons.utils.types import TensorLike, FloatTensorLike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd2372-8194-40e1-a64b-e630b8e96f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5b63e1-1034-4981-98df-b0a89af6b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth exp quantile loss\n",
    "\n",
    "@tf.function\n",
    "def exp_pinball_loss(\n",
    "    y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5,\n",
    "    alpha: FloatTensorLike = 0.001\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes the pinball loss between `y_true` and `y_pred`.\n",
    "    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n",
    "    In the context of regression this loss yields an estimator of the tau\n",
    "    conditional quantile.\n",
    "    See: https://en.wikipedia.org/wiki/Quantile_regression\n",
    "    Usage:\n",
    "    >>> loss = tfa.losses.pinball_loss([0., 0., 1., 1.],\n",
    "    ... [1., 1., 1., 0.], tau=.1)\n",
    "    >>> loss\n",
    "    <tf.Tensor: shape=(), dtype=float32, numpy=0.475>\n",
    "    Args:\n",
    "      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`\n",
    "      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
    "      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n",
    "        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n",
    "        the context of quantile regression, the value of tau determines the\n",
    "        conditional quantile level. When tau = 0.5, this amounts to l1\n",
    "        regression, an estimator of the conditional median (0.5 quantile).\n",
    "    Returns:\n",
    "        pinball_loss: 1-D float `Tensor` with shape [batch_size].\n",
    "    References:\n",
    "      - https://en.wikipedia.org/wiki/Quantile_regression\n",
    "      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n",
    "    \"\"\"\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    alpha = tf.expand_dims(tf.cast(alpha, y_pred.dtype), 0)\n",
    "\n",
    "    delta_y = y_true - y_pred\n",
    "    #Implement smooth loss\n",
    "    pinball = tau * delta_y + alpha * tf.math.softplus(-delta_y/alpha)\n",
    "    return tf.reduce_mean(pinball, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aecbfd-b903-489d-a57f-a43f4ee44bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth sqrt quantile loss\n",
    "\n",
    "@tf.function\n",
    "def sqrt_pinball_loss(\n",
    "    y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5,\n",
    "    alpha: FloatTensorLike = 0.001\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes the pinball loss between `y_true` and `y_pred`.\n",
    "    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n",
    "    In the context of regression this loss yields an estimator of the tau\n",
    "    conditional quantile.\n",
    "    See: https://en.wikipedia.org/wiki/Quantile_regression\n",
    "    Usage:\n",
    "    >>> loss = tfa.losses.pinball_loss([0., 0., 1., 1.],\n",
    "    ... [1., 1., 1., 0.], tau=.1)\n",
    "    >>> loss\n",
    "    <tf.Tensor: shape=(), dtype=float32, numpy=0.475>\n",
    "    Args:\n",
    "      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`\n",
    "      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
    "      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n",
    "        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n",
    "        the context of quantile regression, the value of tau determines the\n",
    "        conditional quantile level. When tau = 0.5, this amounts to l1\n",
    "        regression, an estimator of the conditional median (0.5 quantile).\n",
    "    Returns:\n",
    "        pinball_loss: 1-D float `Tensor` with shape [batch_size].\n",
    "    References:\n",
    "      - https://en.wikipedia.org/wiki/Quantile_regression\n",
    "      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n",
    "    \"\"\"\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    alpha = tf.expand_dims(tf.cast(alpha, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    delta_y = y_true - y_pred\n",
    "    #Implement smooth loss\n",
    "    pinball = (delta_y*(2*tau - one) + tf.math.sqrt(tf.math.square(delta_y) + alpha))/2\n",
    "    return tf.reduce_mean(pinball, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96de0c2-ae0b-4c96-bdba-1dfafeab4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Huber quantile loss\n",
    "\n",
    "@tf.function\n",
    "def huber_pinball_loss(\n",
    "    y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5,\n",
    "    alpha: FloatTensorLike = 0.001\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes the pinball loss between `y_true` and `y_pred`.\n",
    "    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n",
    "    In the context of regression this loss yields an estimator of the tau\n",
    "    conditional quantile.\n",
    "    See: https://en.wikipedia.org/wiki/Quantile_regression\n",
    "    Usage:\n",
    "    >>> loss = tfa.losses.pinball_loss([0., 0., 1., 1.],\n",
    "    ... [1., 1., 1., 0.], tau=.1)\n",
    "    >>> loss\n",
    "    <tf.Tensor: shape=(), dtype=float32, numpy=0.475>\n",
    "    Args:\n",
    "      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`\n",
    "      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
    "      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n",
    "        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n",
    "        the context of quantile regression, the value of tau determines the\n",
    "        conditional quantile level. When tau = 0.5, this amounts to l1\n",
    "        regression, an estimator of the conditional median (0.5 quantile).\n",
    "    Returns:\n",
    "        pinball_loss: 1-D float `Tensor` with shape [batch_size].\n",
    "    References:\n",
    "      - https://en.wikipedia.org/wiki/Quantile_regression\n",
    "      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n",
    "    \"\"\"\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    alpha = tf.expand_dims(tf.cast(alpha, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    error = tf.subtract(y_true,y_pred)\n",
    "    abs_error = tf.abs(error)\n",
    "    half = tf.convert_to_tensor(0.5, dtype=abs_error.dtype)\n",
    "    huber = tf.where(abs_error <= alpha, half * tf.square(error)/alpha,\n",
    "                         abs_error - half * alpha)\n",
    "    \n",
    "    \n",
    "    #Implement smooth loss\n",
    "    pinball = tf.where(error >=0, tau * huber, (one - tau) * huber)\n",
    "    return tf.reduce_mean(pinball, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57753f-8123-4463-8bc9-8e0707d6cb6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tune Wind model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8b7b8-575d-4e27-8127-83f99f5f781e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e87c2d-75e8-4fc3-9aae-6101dbb00f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataframe, label_encoder = None,feature_scaler = None, target_scaler = None, learn = False):\n",
    "    #Drop unused columns\n",
    "    data = dataframe.copy()\n",
    "    data.drop([\"init_tm\", \"met_var\", \"location\",  \"ens_var\", \"obs_tm\"], axis = 1, inplace = True)\n",
    "    data = data.to_numpy()\n",
    "    if learn == True:\n",
    "        label_encoder = LabelEncoder()\n",
    "        feature_scaler = StandardScaler()\n",
    "        target_scaler = StandardScaler()\n",
    "        #Learn label encoding for horizons\n",
    "        label = label_encoder.fit_transform(data[:,0])\n",
    "        #Learn target scaling\n",
    "        target_scaled = target_scaler.fit_transform(data[:,1].reshape(-1,1))\n",
    "        #Learn feature scaling\n",
    "        feature_scaled = feature_scaler.fit_transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data, label_encoder, feature_scaler, target_scaler\n",
    "    \n",
    "    else:\n",
    "        #Learn labels\n",
    "        label = label_encoder.transform(data[:,0])\n",
    "        #Scale target\n",
    "        target_scaled = target_scaler.transform(data[:,1].reshape(-1,1))\n",
    "        #Scale features\n",
    "        feature_scaled = feature_scaler.transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57784577-06ee-46fc-90fd-553536e0793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(input_data, predict = False):\n",
    "    #Extract forecast embedding\n",
    "    horizon_emb = input_data[:,0]\n",
    "    \n",
    "    if predict == False:        \n",
    "        #Extract features\n",
    "        features = input_data[:,2:]\n",
    "        # Extract target\n",
    "        target = np.expand_dims(input_data[:,1],1)\n",
    "        return [features, horizon_emb], target\n",
    "    else:\n",
    "        #Extract features\n",
    "        features = input_data[:,1:]\n",
    "        return [features, horizon_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3de855-a07a-44dd-b2b1-0415c0d27655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, train_target, validation_data, batch_size, epochs, learning_rate, fine_tuning = True):\n",
    "    model = base_model()    \n",
    "    #Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    #Callbacks\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "    model.compile(optimizer = optimizer, loss = lambda true,pred: pinball_loss(true, pred, tau = quantiles))\n",
    "    #model.compile(optimizer = optimizer, loss = lambda true,pred: smooth_pinball_loss(true, pred, tau = quantiles))\n",
    "    #Normal fit\n",
    "    history1 = model.fit(x = train_data, y = train_target, validation_data = validation_data, epochs = epochs, batch_size = BATCH_SIZE, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    \n",
    "    #Fine tuning\n",
    "    if fine_tuning == True:\n",
    "        enc_horizons = label_encoder.transform(horizons)\n",
    "        train_filtering = np.isin(train_data[1], enc_horizons)\n",
    "        train_data_fine = [train_data[0][train_filtering], train_data[1][train_filtering]]\n",
    "        train_target_fine = train_target[train_filtering]\n",
    "        #Val filtering\n",
    "        val_data, val_target = validation_data\n",
    "        val_filtering = np.isin(val_data[1], enc_horizons)\n",
    "        val_data_fine = [val_data[0][val_filtering], val_data[1][val_filtering]]\n",
    "        val_target_fine = val_target[val_filtering]\n",
    "        validation_data_fine = (val_data_fine, val_target_fine)\n",
    "        \n",
    "        #New optimizer\n",
    "        history2 = model.fit(x = train_data_fine, y = train_target_fine, validation_data = validation_data_fine, epochs = epochs, batch_size = 256, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    return model, [history1, history2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c92120-2ab7-4e20-a369-f4a180604891",
   "metadata": {},
   "source": [
    "## Read data and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47a5f670-d216-4e78-b696-2248228501f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "horizons = [36, 48 ,60, 72, 84]\n",
    "n_encodings = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "441417c7-2bec-4a66-9d44-c89a37801b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind data\n",
    "wind_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_wind_10m.feather\")\n",
    "#Pressure data\n",
    "pressure_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_mslp.feather\")\n",
    "pressure_data.rename({\"ens_mean\":\"mean_pressure\"}, axis = 1, inplace = True)\n",
    "#Cloud data\n",
    "cloud_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_clct.feather\")\n",
    "cloud_data.rename({\"ens_mean\":\"cloud_coverage\"}, axis = 1, inplace = True)\n",
    "#Vmax data\n",
    "max_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_vmax_10m.feather\")\n",
    "max_data.rename({\"ens_mean\":\"vmax\"}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "data = wind_data.merge(pressure_data[[\"init_tm\",\"fcst_hour\",\"mean_pressure\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(cloud_data[[\"init_tm\",\"fcst_hour\",\"cloud_coverage\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(max_data[[\"init_tm\",\"fcst_hour\",\"vmax\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "#Replace vmax NaNs by mean\n",
    "vmax_mean = data[\"vmax\"].mean()\n",
    "data.loc[:,\"vmax\"].fillna(vmax_mean, inplace = True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "#Positional encoding\n",
    "pos_enc = pd.DataFrame(index=pd.DatetimeIndex(data[\"obs_tm\"]))\n",
    "pos_enc[\"Dayofyear\"] = pos_enc.index.dayofyear\n",
    "pos_enc[\"n_days\"] = 365\n",
    "pos_enc.loc[pos_enc.index.year==2020,\"n_days\"] = 366\n",
    "#Calculate actual positional encoding\n",
    "cos_encoding = np.cos(2*math.pi*pos_enc[\"Dayofyear\"]/pos_enc[\"n_days\"])\n",
    "data[\"pos_enc_1\"] = cos_encoding.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd2367-25dd-444a-a111-c8b7abc224e8",
   "metadata": {},
   "source": [
    "## Create study object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dab120e-a398-4eda-98b7-a07aff811d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_index, test_index, data):\n",
    "    #Get split\n",
    "    train_df = data.loc[train_index]\n",
    "    test_df = data.loc[test_index]\n",
    "\n",
    "    #Normalize data\n",
    "    train, label_encoder, feature_scaler, target_scaler = normalize(train_df, learn = True)\n",
    "    test = normalize(test_df, label_encoder, feature_scaler, target_scaler)\n",
    "    n_encodings = len(np.unique(train[:,0]))\n",
    "\n",
    "    #Convert format\n",
    "    train_data, train_target = convert_format(train)\n",
    "    test_data, test_target = convert_format(test)\n",
    "    \n",
    "    return train_data, train_target, test_data, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d997f6ce-c20c-4fed-86b7-988b6891fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(trial, quantiles = quantiles):\n",
    "    #Sample alpha\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-5, 0.01)\n",
    "    loss = trial.suggest_categorical(\"loss\", [\"pinball\",\"exp\",\"abs\",\"huber\"])\n",
    "    losses = {\n",
    "    \"pinball\": lambda true,pred: pinball_loss(true, pred, tau = quantiles),\n",
    "    \"exp\": lambda true,pred: exp_pinball_loss(true, pred, tau = quantiles, alpha = alpha),\n",
    "    \"abs\": lambda true,pred: sqrt_pinball_loss(true, pred, tau = quantiles, alpha = alpha),\n",
    "    \"huber\": lambda true,pred: huber_pinball_loss(true, pred, tau = quantiles, alpha = alpha)}\n",
    "\n",
    "\n",
    "    loss_func = losses[loss]\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f381d58c-7ac9-4331-a9c7-1c4157daf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(trial):\n",
    "    # Copied from optuna tutorial\n",
    "    kwargs = {}\n",
    "    optimizer_options = [\"Adam\", \"SGD\"]\n",
    "    optimizer_selected = trial.suggest_categorical(\"optimizer\", optimizer_options)\n",
    "    if optimizer_selected == \"Adam\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    elif optimizer_selected == \"SGD\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\n",
    "            \"sgd_opt_learning_rate\", 1e-5, 1e-2, log=True\n",
    "        )\n",
    "    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5d06273e-d926-431b-b55b-c500d4c2c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    #Parameters\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.05, 0.5, log = True)\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 2, 6, step = 2)\n",
    "    n_layers = trial.suggest_int(\"n_layers\",1,2,1)\n",
    "    n_units_1 = trial.suggest_int(\"n_units_1\",16,128, log =True)\n",
    "    if n_layers == 2:\n",
    "        n_units_2 = trial.suggest_int(\"n_units_2\", 16, 128, log = True)\n",
    "    else:\n",
    "        n_units_2 = 1\n",
    "    #Create Model\n",
    "    class base_model(tf.keras.Model):    \n",
    "        def __init__(self, n_layers, n_units_1, n_units_2, embedding_dim, dropout_rate, n_embeddings = n_encodings):\n",
    "            super(base_model, self).__init__()\n",
    "            #Embedding layers\n",
    "            self.embedding = Embedding(input_dim = n_embeddings, output_dim = embedding_dim)\n",
    "            #N_layers\n",
    "            self.n_layers = n_layers\n",
    "            #Dropout\n",
    "            self.dropout = Dropout(dropout_rate)\n",
    "            #Create Dense layers\n",
    "            self.hidden = Dense(n_units_1, activation = \"relu\")\n",
    "            self.hidden2 = Dense(n_units_2, activation = \"relu\")\n",
    "            self.out = Dense(5, activation = \"linear\")\n",
    "\n",
    "        def call(self, input_data):\n",
    "            #Extract data\n",
    "            features, horizon_emb = input_data\n",
    "            #Calculate embedding\n",
    "            emb = self.embedding(horizon_emb)\n",
    "            emb = tf.squeeze(emb, axis = 1)\n",
    "            conc = Concatenate(axis = 1)([features, emb])\n",
    "            #Calculate output\n",
    "            output = self.hidden(conc)\n",
    "            if self.n_layers == 2:\n",
    "                output = self.hidden2(output)\n",
    "            output = self.dropout(output)\n",
    "            output = self.out(output)\n",
    "            return output\n",
    "\n",
    "    #Train\n",
    "    model = base_model(n_layers, n_units_1, n_units_2, embedding_dim, dropout_rate)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "642a485b-9855-47bb-ab96-107e0803aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, loss_func, optimizer, BATCH_SIZE, EPOCHS = 100,  data = data, n_splits = 10):\n",
    "    #Get data\n",
    "    data = data.reset_index().drop(\"index\",axis=1)\n",
    "    fold = KFold(n_splits = n_splits, shuffle = True, random_state = 10)\n",
    "    split = fold.split(data.index)\n",
    "    \n",
    "    #Total loss\n",
    "    test_loss = 0\n",
    "    for train_index, test_index in split:\n",
    "        #Get data\n",
    "        train_data, train_target, test_data, test_target = get_data(train_index, test_index, data)\n",
    "\n",
    "        #Compile model\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "        model.compile(optimizer = optimizer, loss = loss_func)\n",
    "        \n",
    "        #Normal fit\n",
    "        history1 = model.fit(x = train_data, y = train_target, validation_split = 0.2, epochs = EPOCHS, batch_size = BATCH_SIZE, callbacks = [callback], shuffle = True, verbose = False)\n",
    "\n",
    "        #Calculate loss\n",
    "        pred = model.predict(test_data)\n",
    "        total_loss = 0\n",
    "        for cnt,quantile in enumerate(quantiles):\n",
    "            loss = mean_pinball_loss(test_target.reshape(-1), pred[:,cnt].reshape(-1), alpha = quantile)\n",
    "            total_loss += loss\n",
    "\n",
    "        test_loss += total_loss/len(quantiles)\n",
    "\n",
    "    return test_loss/n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a288a49-3b39-406f-9139-0498184a769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Create Parameters\n",
    "    BATCH_SIZE = 2**trial.suggest_int(\"batch_size\",7,10,1)\n",
    "    EPOCHS = 100\n",
    "    optimizer = get_optimizer(trial)    \n",
    "    #Get loss\n",
    "    loss_func = get_loss(trial)        \n",
    "    #Get Model\n",
    "    model = create_model(trial)    \n",
    "    #Train model\n",
    "    loss = learn(model = model, loss_func = loss_func, optimizer = optimizer, BATCH_SIZE = BATCH_SIZE, EPOCHS = EPOCHS, n_splits = 10)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abad412-75b4-45db-95e8-860055b5fd2d",
   "metadata": {},
   "source": [
    "## Run study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d9a4da80-a95c-43f4-acf0-6569c52564d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-07 10:53:37,667]\u001b[0m A new study created in memory with name: no-name-660a88f7-9377-468e-8ab4-38ee336575a9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split finished\n",
      "Split finished\n",
      "Split finished\n",
      "Split finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-07 10:58:57,452]\u001b[0m Trial 0 finished with value: 0.13386053716326937 and parameters: {'batch_size': 7, 'optimizer': 'SGD', 'sgd_opt_learning_rate': 0.003032461510449239, 'sgd_opt_momentum': 0.005137794061527512, 'alpha': 0.00024240558891133236, 'loss': 'huber', 'dropout': 0.2821537424639687, 'n_layers': 2, 'n_units_1': 75, 'n_units_2': 112}. Best is trial 0 with value: 0.13386053716326937.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split finished\n",
      "Split finished\n",
      "Split finished\n",
      "Split finished\n",
      "Split finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-07 11:00:02,592]\u001b[0m Trial 1 finished with value: 0.13944635022300297 and parameters: {'batch_size': 10, 'optimizer': 'SGD', 'sgd_opt_learning_rate': 0.02414933525460908, 'sgd_opt_momentum': 0.0006118796446215753, 'alpha': 0.004837564092016341, 'loss': 'huber', 'dropout': 0.5704457896329578, 'n_layers': 1, 'n_units_1': 22}. Best is trial 0 with value: 0.13386053716326937.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split finished\n"
     ]
    }
   ],
   "source": [
    "wind_study = opt.create_study(direction = 'minimize')\n",
    "wind_study.optimize(objective, n_trials = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85cd0668-f29f-4d73-acaa-60b9ca7e83e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19690207430330064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 8,\n",
       " 'optimizer': 'SGD',\n",
       " 'sgd_opt_learning_rate': 0.0010313188174217675,\n",
       " 'sgd_opt_momentum': 0.004165680478234925,\n",
       " 'alpha': 0.004277917581196242,\n",
       " 'loss': 'exp',\n",
       " 'dropout': 0.47635102537779317,\n",
       " 'n_layers': 1,\n",
       " 'n_units_1': 82}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wind_study.best_value)\n",
    "wind_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4d824-cd5c-4951-a81c-eadb3614b929",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tune temperature model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07825ebd-1bce-418f-a954-ea230a554fc7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ed0b17bb-1b86-4351-9da8-b87f005a2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_t_2m.feather\")\n",
    "data.dropna(inplace=True)\n",
    "#Positional encoding\n",
    "pos_enc = pd.DataFrame(index=pd.DatetimeIndex(data[\"obs_tm\"]))\n",
    "pos_enc[\"Dayofyear\"] = pos_enc.index.dayofyear\n",
    "pos_enc[\"n_days\"] = 365\n",
    "pos_enc.loc[pos_enc.index.year==2020,\"n_days\"] = 366\n",
    "#Calculate actual positional encoding\n",
    "cos_encoding = np.cos(2*math.pi*pos_enc[\"Dayofyear\"]/pos_enc[\"n_days\"])\n",
    "data[\"pos_enc_1\"] = cos_encoding.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7b381-03b4-4663-a35e-18d5fdf0d243",
   "metadata": {},
   "source": [
    "## Run study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af17ac63-2614-4810-b9a6-461ea97a3c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-07 11:11:00,226]\u001b[0m A new study created in memory with name: no-name-248eab70-63a4-4812-8b65-cc660f45e6ce\u001b[0m\n",
      "\u001b[32m[I 2022-01-07 11:12:08,702]\u001b[0m Trial 0 finished with value: 0.1328046804259026 and parameters: {'batch_size': 10, 'optimizer': 'SGD', 'sgd_opt_learning_rate': 0.038440309274078224, 'sgd_opt_momentum': 9.17496850869131e-05, 'alpha': 0.0022421353940460785, 'loss': 'pinball', 'dropout': 0.0653511660886827, 'embedding_dim': 2, 'n_layers': 2, 'n_units_1': 73, 'n_units_2': 35}. Best is trial 0 with value: 0.1328046804259026.\u001b[0m\n",
      "\u001b[32m[I 2022-01-07 11:19:44,380]\u001b[0m Trial 1 finished with value: 0.6002262259077483 and parameters: {'batch_size': 7, 'optimizer': 'RMSprop', 'rmsprop_learning_rate': 9.895175597813428e-05, 'rmsprop_decay': 0.945844097646751, 'rmsprop_momentum': 0.0029053953305711684, 'alpha': 0.00644961323372858, 'loss': 'abs', 'dropout': 0.09046882877784096, 'embedding_dim': 2, 'n_layers': 1, 'n_units_1': 23}. Best is trial 0 with value: 0.1328046804259026.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "temp_study = opt.create_study(direction = 'minimize')\n",
    "temp_study.optimize(objective, n_trials = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24ec4e-8616-4ad0-8786-ac9dedcfaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_study.best_value)\n",
    "temp_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40441c18-946f-4e1e-9c6d-6a142716b042",
   "metadata": {},
   "source": [
    "# Compare Loss functions\n",
    "\n",
    "Compare Loss functions by tuning only loss function for wind model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b494cd2-6b7d-446f-ac92-d49ae1fbcd31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f31067f-ca12-45a9-83a8-69e175aad714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataframe, label_encoder = None,feature_scaler = None, target_scaler = None, learn = False):\n",
    "    #Drop unused columns\n",
    "    data = dataframe.copy()\n",
    "    data.drop([\"init_tm\", \"met_var\", \"location\",  \"ens_var\", \"obs_tm\"], axis = 1, inplace = True)\n",
    "    data = data.to_numpy()\n",
    "    if learn == True:\n",
    "        label_encoder = LabelEncoder()\n",
    "        feature_scaler = StandardScaler()\n",
    "        target_scaler = StandardScaler()\n",
    "        #Learn label encoding for horizons\n",
    "        label = label_encoder.fit_transform(data[:,0])\n",
    "        #Learn target scaling\n",
    "        target_scaled = target_scaler.fit_transform(data[:,1].reshape(-1,1))\n",
    "        #Learn feature scaling\n",
    "        feature_scaled = feature_scaler.fit_transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data, label_encoder, feature_scaler, target_scaler\n",
    "    \n",
    "    else:\n",
    "        #Learn labels\n",
    "        label = label_encoder.transform(data[:,0])\n",
    "        #Scale target\n",
    "        target_scaled = target_scaler.transform(data[:,1].reshape(-1,1))\n",
    "        #Scale features\n",
    "        feature_scaled = feature_scaler.transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9d34a5-5ca9-4a3e-9ffc-d55c977731c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(input_data, predict = False):\n",
    "    #Extract forecast embedding\n",
    "    horizon_emb = input_data[:,0]\n",
    "    \n",
    "    if predict == False:        \n",
    "        #Extract features\n",
    "        features = input_data[:,2:]\n",
    "        # Extract target\n",
    "        target = np.expand_dims(input_data[:,1],1)\n",
    "        return [features, horizon_emb], target\n",
    "    else:\n",
    "        #Extract features\n",
    "        features = input_data[:,1:]\n",
    "        return [features, horizon_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c1a2a9-2803-4434-9f74-99878c3f6c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, train_target, validation_data, batch_size, epochs, learning_rate, fine_tuning = True):\n",
    "    model = base_model()    \n",
    "    #Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    #Callbacks\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "    model.compile(optimizer = optimizer, loss = lambda true,pred: pinball_loss(true, pred, tau = quantiles))\n",
    "    #model.compile(optimizer = optimizer, loss = lambda true,pred: smooth_pinball_loss(true, pred, tau = quantiles))\n",
    "    #Normal fit\n",
    "    history1 = model.fit(x = train_data, y = train_target, validation_data = validation_data, epochs = epochs, batch_size = BATCH_SIZE, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    \n",
    "    #Fine tuning\n",
    "    if fine_tuning == True:\n",
    "        enc_horizons = label_encoder.transform(horizons)\n",
    "        train_filtering = np.isin(train_data[1], enc_horizons)\n",
    "        train_data_fine = [train_data[0][train_filtering], train_data[1][train_filtering]]\n",
    "        train_target_fine = train_target[train_filtering]\n",
    "        #Val filtering\n",
    "        val_data, val_target = validation_data\n",
    "        val_filtering = np.isin(val_data[1], enc_horizons)\n",
    "        val_data_fine = [val_data[0][val_filtering], val_data[1][val_filtering]]\n",
    "        val_target_fine = val_target[val_filtering]\n",
    "        validation_data_fine = (val_data_fine, val_target_fine)\n",
    "        \n",
    "        #New optimizer\n",
    "        history2 = model.fit(x = train_data_fine, y = train_target_fine, validation_data = validation_data_fine, epochs = epochs, batch_size = 256, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    return model, [history1, history2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5db94a-5974-4fbb-9419-35bf625dbb99",
   "metadata": {},
   "source": [
    "## Read data and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87bc3f41-60c1-4eaa-9368-6de794120821",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "horizons = [36, 48 ,60, 72, 84]\n",
    "n_encodings = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84630e3f-5acb-4a53-b430-620779167185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind data\n",
    "wind_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_wind_10m.feather\")\n",
    "#Pressure data\n",
    "pressure_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_mslp.feather\")\n",
    "pressure_data.rename({\"ens_mean\":\"mean_pressure\"}, axis = 1, inplace = True)\n",
    "#Cloud data\n",
    "cloud_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_clct.feather\")\n",
    "cloud_data.rename({\"ens_mean\":\"cloud_coverage\"}, axis = 1, inplace = True)\n",
    "#Vmax data\n",
    "max_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_vmax_10m.feather\")\n",
    "max_data.rename({\"ens_mean\":\"vmax\"}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "data = wind_data.merge(pressure_data[[\"init_tm\",\"fcst_hour\",\"mean_pressure\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(cloud_data[[\"init_tm\",\"fcst_hour\",\"cloud_coverage\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(max_data[[\"init_tm\",\"fcst_hour\",\"vmax\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "#Replace vmax NaNs by mean\n",
    "vmax_mean = data[\"vmax\"].mean()\n",
    "data.loc[:,\"vmax\"].fillna(vmax_mean, inplace = True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "#Positional encoding\n",
    "pos_enc = pd.DataFrame(index=pd.DatetimeIndex(data[\"obs_tm\"]))\n",
    "pos_enc[\"Dayofyear\"] = pos_enc.index.dayofyear\n",
    "pos_enc[\"n_days\"] = 365\n",
    "pos_enc.loc[pos_enc.index.year==2020,\"n_days\"] = 366\n",
    "#Calculate actual positional encoding\n",
    "cos_encoding = np.cos(2*math.pi*pos_enc[\"Dayofyear\"]/pos_enc[\"n_days\"])\n",
    "data[\"pos_enc_1\"] = cos_encoding.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4952b5-14fa-4dac-b706-a9525cf6c74b",
   "metadata": {},
   "source": [
    "## Create study object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea16ca0-9d42-4488-af4f-59ada45cc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_index, test_index, data):\n",
    "    #Get split\n",
    "    train_df = data.loc[train_index]\n",
    "    test_df = data.loc[test_index]\n",
    "\n",
    "    #Normalize data\n",
    "    train, label_encoder, feature_scaler, target_scaler = normalize(train_df, learn = True)\n",
    "    test = normalize(test_df, label_encoder, feature_scaler, target_scaler)\n",
    "    n_encodings = len(np.unique(train[:,0]))\n",
    "\n",
    "    #Convert format\n",
    "    train_data, train_target = convert_format(train)\n",
    "    test_data, test_target = convert_format(test)\n",
    "    \n",
    "    return train_data, train_target, test_data, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c193b09-c0fb-423f-a905-e5e8d315c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss, alpha, quantiles = quantiles):\n",
    "    #Sample alpha\n",
    "    losses = {\n",
    "    \"pinball\": lambda true,pred: pinball_loss(true, pred, tau = quantiles),\n",
    "    \"exp\": lambda true,pred: exp_pinball_loss(true, pred, tau = quantiles, alpha = alpha),\n",
    "    \"abs\": lambda true,pred: sqrt_pinball_loss(true, pred, tau = quantiles, alpha = alpha),\n",
    "    \"huber\": lambda true,pred: huber_pinball_loss(true, pred, tau = quantiles, alpha = alpha)\n",
    "    }\n",
    "    loss_func = losses[loss]\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "472dfaba-4146-467b-8a2b-d53f8caa33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    #Parameters\n",
    "    dropout_rate = 0.1\n",
    "    n_layers = 2\n",
    "    n_units_1 = 58\n",
    "    if n_layers == 2:\n",
    "        n_units_2 = 64\n",
    "    else:\n",
    "        n_units_2 = 1\n",
    "    #Create Model\n",
    "    class base_model(tf.keras.Model):    \n",
    "        def __init__(self, n_layers, n_units_1, n_units_2, dropout_rate, n_embeddings = n_encodings):\n",
    "            super(base_model, self).__init__()\n",
    "            #Embedding layers\n",
    "            self.embedding = Embedding(input_dim = n_embeddings, output_dim = 4)\n",
    "            #N_layers\n",
    "            self.n_layers = n_layers\n",
    "            #Dropout\n",
    "            self.dropout = Dropout(dropout_rate)\n",
    "            #Create Dense layers\n",
    "            self.hidden = Dense(n_units_1, activation = \"relu\")\n",
    "            self.hidden2 = Dense(n_units_2, activation = \"relu\")\n",
    "            self.out = Dense(5, activation = \"linear\")\n",
    "\n",
    "        def call(self, input_data):\n",
    "            #Extract data\n",
    "            features, horizon_emb = input_data\n",
    "            #Calculate embedding\n",
    "            emb = self.embedding(horizon_emb)\n",
    "            emb = tf.squeeze(emb, axis = 1)\n",
    "            conc = Concatenate(axis = 1)([features, emb])\n",
    "            #Calculate output\n",
    "            output = self.hidden(conc)\n",
    "            if self.n_layers == 2:\n",
    "                output = self.hidden2(output)\n",
    "            output = self.dropout(output)\n",
    "            output = self.out(output)\n",
    "            return output\n",
    "\n",
    "    #Train\n",
    "    model = base_model(n_layers, n_units_1, n_units_2, dropout_rate)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6e0959e-b081-4537-9e06-8f158ca3df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, loss_func, optimizer, BATCH_SIZE, EPOCHS = 100,  data = data, n_splits = 10):\n",
    "    #Get data\n",
    "    data = data.reset_index().drop(\"index\",axis=1)\n",
    "    fold = KFold(n_splits = n_splits, shuffle = True, random_state = 10)\n",
    "    split = fold.split(data.index)\n",
    "    \n",
    "    #Total loss\n",
    "    test_loss = 0\n",
    "    for train_index, test_index in split:\n",
    "        #Get data\n",
    "        train_data, train_target, test_data, test_target = get_data(train_index, test_index, data)\n",
    "\n",
    "        #Compile model\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "        model.compile(optimizer = optimizer, loss = loss_func)\n",
    "        \n",
    "        #Normal fit\n",
    "        history1 = model.fit(x = train_data, y = train_target, validation_split = 0.2, epochs = EPOCHS, batch_size = BATCH_SIZE, callbacks = [callback], shuffle = True, verbose = False)\n",
    "\n",
    "        #Calculate loss\n",
    "        pred = model.predict(test_data)\n",
    "        total_loss = 0\n",
    "        for cnt,quantile in enumerate(quantiles):\n",
    "            loss = mean_pinball_loss(test_target.reshape(-1), pred[:,cnt].reshape(-1), alpha = quantile)\n",
    "            total_loss += loss\n",
    "\n",
    "        test_loss += total_loss/len(quantiles)\n",
    "\n",
    "    return test_loss/n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b518ba8-596a-4c79-8b09-0598b11258f2",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcfa2aad-91ec-49ba-836f-ed51ca75cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters\n",
    "alpha_space = np.logspace(start = np.log10(1e-5), stop = np.log10(1), num = 30)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "663d33f0-7d4e-42e0-b898-87c419bd92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataframe\n",
    "results = pd.DataFrame(index = np.arange(120), columns = [\"Method\", \"alpha\", \"Pinball loss\"])\n",
    "\n",
    "for cnt_loss, loss in enumerate([\"pinball\",\"exp\",\"abs\",\"huber\"]):\n",
    "    for cnt_alpha,alpha in enumerate(alpha_space):\n",
    "        loss_func = get_loss(loss,alpha)    \n",
    "        model = create_model() \n",
    "        pinball_loss = learn(model = model, loss_func = loss_func, optimizer = optimizer, BATCH_SIZE = BATCH_SIZE, EPOCHS = EPOCHS, n_splits = 5)\n",
    "        results.loc[cnt_loss*30+cnt_alpha] = [loss, alpha, pinball_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5cce2-dc31-4b6e-ab77-dd1aed709ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
