{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2684caa-a520-4b33-909a-fab7d08346a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler, LabelEncoder\n",
    "from tensorflow_addons.losses import pinball_loss\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from scipy import stats\n",
    "import math\n",
    "import optuna as opt\n",
    "\n",
    "from typeguard import typechecked\n",
    "from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
    "from tensorflow_addons.utils.types import TensorLike, FloatTensorLike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd2372-8194-40e1-a64b-e630b8e96f1f",
   "metadata": {},
   "source": [
    "# Create losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f5b63e1-1034-4981-98df-b0a89af6b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth quantile loss\n",
    "\n",
    "@tf.function\n",
    "def smooth_pinball_loss(\n",
    "    y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5,\n",
    "    alpha: FloatTensorLike = 0.001\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes the pinball loss between `y_true` and `y_pred`.\n",
    "    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n",
    "    In the context of regression this loss yields an estimator of the tau\n",
    "    conditional quantile.\n",
    "    See: https://en.wikipedia.org/wiki/Quantile_regression\n",
    "    Usage:\n",
    "    >>> loss = tfa.losses.pinball_loss([0., 0., 1., 1.],\n",
    "    ... [1., 1., 1., 0.], tau=.1)\n",
    "    >>> loss\n",
    "    <tf.Tensor: shape=(), dtype=float32, numpy=0.475>\n",
    "    Args:\n",
    "      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`\n",
    "      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
    "      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n",
    "        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n",
    "        the context of quantile regression, the value of tau determines the\n",
    "        conditional quantile level. When tau = 0.5, this amounts to l1\n",
    "        regression, an estimator of the conditional median (0.5 quantile).\n",
    "    Returns:\n",
    "        pinball_loss: 1-D float `Tensor` with shape [batch_size].\n",
    "    References:\n",
    "      - https://en.wikipedia.org/wiki/Quantile_regression\n",
    "      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n",
    "    \"\"\"\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "\n",
    "    delta_y = y_true - y_pred\n",
    "    #Implement smooth loss\n",
    "    pinball = tau * delta_y + alpha * tf.math.softplus(-delta_y/alpha)\n",
    "    return tf.reduce_mean(pinball, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c96de0c2-ae0b-4c96-bdba-1dfafeab4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Huber quantile loss\n",
    "\n",
    "@tf.function\n",
    "def huber_pinball_loss(\n",
    "    y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5,\n",
    "    delta: FloatTensorLike = 0.01\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes the pinball loss between `y_true` and `y_pred`.\n",
    "    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n",
    "    In the context of regression this loss yields an estimator of the tau\n",
    "    conditional quantile.\n",
    "    See: https://en.wikipedia.org/wiki/Quantile_regression\n",
    "    Usage:\n",
    "    >>> loss = tfa.losses.pinball_loss([0., 0., 1., 1.],\n",
    "    ... [1., 1., 1., 0.], tau=.1)\n",
    "    >>> loss\n",
    "    <tf.Tensor: shape=(), dtype=float32, numpy=0.475>\n",
    "    Args:\n",
    "      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`\n",
    "      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
    "      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n",
    "        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n",
    "        the context of quantile regression, the value of tau determines the\n",
    "        conditional quantile level. When tau = 0.5, this amounts to l1\n",
    "        regression, an estimator of the conditional median (0.5 quantile).\n",
    "    Returns:\n",
    "        pinball_loss: 1-D float `Tensor` with shape [batch_size].\n",
    "    References:\n",
    "      - https://en.wikipedia.org/wiki/Quantile_regression\n",
    "      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n",
    "    \"\"\"\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    delta = tf.expand_dims(tf.cast(delta, y_pred.dtype), 0)\n",
    "\n",
    "    error = tf.subtract(y_true,y_pred)\n",
    "    abs_error = tf.abs(error)\n",
    "    half = tf.convert_to_tensor(0.5, dtype=abs_error.dtype)\n",
    "    huber = tf.where(abs_error <= delta, half * tf.square(error),\n",
    "                         abs_error - half * tf.square(delta))\n",
    "    \n",
    "    \n",
    "    #Implement smooth loss\n",
    "    pinball = tf.where(abs_error >= 0, tau * huber, (tau - 1)*huber)\n",
    "    return tf.reduce_mean(pinball, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57753f-8123-4463-8bc9-8e0707d6cb6b",
   "metadata": {},
   "source": [
    "# Tune Wind model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8b7b8-575d-4e27-8127-83f99f5f781e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e87c2d-75e8-4fc3-9aae-6101dbb00f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataframe, label_encoder = None,feature_scaler = None, target_scaler = None, learn = False):\n",
    "    #Drop unused columns\n",
    "    data = dataframe.copy()\n",
    "    data.drop([\"init_tm\", \"met_var\", \"location\",  \"ens_var\", \"obs_tm\"], axis = 1, inplace = True)\n",
    "    data = data.to_numpy()\n",
    "    if learn == True:\n",
    "        label_encoder = LabelEncoder()\n",
    "        feature_scaler = StandardScaler()\n",
    "        target_scaler = StandardScaler()\n",
    "        #Learn label encoding for horizons\n",
    "        label = label_encoder.fit_transform(data[:,0])\n",
    "        #Learn target scaling\n",
    "        target_scaled = target_scaler.fit_transform(data[:,1].reshape(-1,1))\n",
    "        #Learn feature scaling\n",
    "        feature_scaled = feature_scaler.fit_transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data, label_encoder, feature_scaler, target_scaler\n",
    "    \n",
    "    else:\n",
    "        #Learn labels\n",
    "        label = label_encoder.transform(data[:,0])\n",
    "        #Scale target\n",
    "        target_scaled = target_scaler.transform(data[:,1].reshape(-1,1))\n",
    "        #Scale features\n",
    "        feature_scaled = feature_scaler.transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57784577-06ee-46fc-90fd-553536e0793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(input_data, predict = False):\n",
    "    #Extract forecast embedding\n",
    "    horizon_emb = input_data[:,0]\n",
    "    \n",
    "    if predict == False:        \n",
    "        #Extract features\n",
    "        features = input_data[:,2:]\n",
    "        # Extract target\n",
    "        target = np.expand_dims(input_data[:,1],1)\n",
    "        return [features, horizon_emb], target\n",
    "    else:\n",
    "        #Extract features\n",
    "        features = input_data[:,1:]\n",
    "        return [features, horizon_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3de855-a07a-44dd-b2b1-0415c0d27655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, train_target, validation_data, batch_size, epochs, learning_rate, fine_tuning = True):\n",
    "    model = base_model()    \n",
    "    #Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    #Callbacks\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "    model.compile(optimizer = optimizer, loss = lambda true,pred: pinball_loss(true, pred, tau = quantiles))\n",
    "    #model.compile(optimizer = optimizer, loss = lambda true,pred: smooth_pinball_loss(true, pred, tau = quantiles))\n",
    "    #Normal fit\n",
    "    history1 = model.fit(x = train_data, y = train_target, validation_data = validation_data, epochs = epochs, batch_size = BATCH_SIZE, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    \n",
    "    #Fine tuning\n",
    "    if fine_tuning == True:\n",
    "        enc_horizons = label_encoder.transform(horizons)\n",
    "        train_filtering = np.isin(train_data[1], enc_horizons)\n",
    "        train_data_fine = [train_data[0][train_filtering], train_data[1][train_filtering]]\n",
    "        train_target_fine = train_target[train_filtering]\n",
    "        #Val filtering\n",
    "        val_data, val_target = validation_data\n",
    "        val_filtering = np.isin(val_data[1], enc_horizons)\n",
    "        val_data_fine = [val_data[0][val_filtering], val_data[1][val_filtering]]\n",
    "        val_target_fine = val_target[val_filtering]\n",
    "        validation_data_fine = (val_data_fine, val_target_fine)\n",
    "        \n",
    "        #New optimizer\n",
    "        history2 = model.fit(x = train_data_fine, y = train_target_fine, validation_data = validation_data_fine, epochs = epochs, batch_size = 256, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    return model, [history1, history2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12db2e3-3ae3-45f6-86ea-837095800f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_training(train_data, train_target, validation_data, test_data,  BATCH_SIZE, EPOCHS, learning_rate, n = 10):\n",
    "    predictions = np.zeros(shape = (len(test_data[0]),5))\n",
    "    for i in range(n):\n",
    "        model,_ = train_model(train_data, train_target, validation_data, BATCH_SIZE, EPOCHS, learning_rate)\n",
    "        pred = model.predict(test_data)\n",
    "        predictions += pred\n",
    "        print(\"Finished Training {}\".format(i+1))\n",
    "    predictions = predictions/n\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c92120-2ab7-4e20-a369-f4a180604891",
   "metadata": {},
   "source": [
    "## Read data and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5f670-d216-4e78-b696-2248228501f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "horizons = [36, 48 ,60, 72, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441417c7-2bec-4a66-9d44-c89a37801b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind data\n",
    "wind_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_wind_10m.feather\")\n",
    "#Pressure data\n",
    "pressure_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_mslp.feather\")\n",
    "pressure_data.rename({\"ens_mean\":\"mean_pressure\"}, axis = 1, inplace = True)\n",
    "#Cloud data\n",
    "cloud_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_clct.feather\")\n",
    "cloud_data.rename({\"ens_mean\":\"cloud_coverage\"}, axis = 1, inplace = True)\n",
    "#Vmax data\n",
    "max_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_vmax_10m.feather\")\n",
    "max_data.rename({\"ens_mean\":\"vmax\"}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "data = wind_data.merge(pressure_data[[\"init_tm\",\"fcst_hour\",\"mean_pressure\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(cloud_data[[\"init_tm\",\"fcst_hour\",\"cloud_coverage\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(max_data[[\"init_tm\",\"fcst_hour\",\"vmax\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "#Replace vmax NaNs by mean\n",
    "vmax_mean = data[\"vmax\"].mean()\n",
    "data.loc[:,\"vmax\"].fillna(vmax_mean, inplace = True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "#Positional encoding\n",
    "pos_enc = pd.DataFrame(index=pd.DatetimeIndex(data[\"obs_tm\"]))\n",
    "pos_enc[\"Dayofyear\"] = pos_enc.index.dayofyear\n",
    "pos_enc[\"n_days\"] = 365\n",
    "pos_enc.loc[pos_enc.index.year==2020,\"n_days\"] = 366\n",
    "#Calculate actual positional encoding\n",
    "cos_encoding = np.cos(2*math.pi*pos_enc[\"Dayofyear\"]/pos_enc[\"n_days\"])\n",
    "data[\"pos_enc_1\"] = cos_encoding.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd2367-25dd-444a-a111-c8b7abc224e8",
   "metadata": {},
   "source": [
    "## Create study object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00533402-de03-4326-9eda-58342328a2fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17ac63-2614-4810-b9a6-461ea97a3c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
