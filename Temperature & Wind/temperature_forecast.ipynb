{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dcc675b-a528-47d8-817d-66cd81b07b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Model\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.math import erf\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler, LabelEncoder\n",
    "from tensorflow_addons.losses import pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9d7181-beab-4222-974f-a5046a47ccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4510d11-04a6-4c31-a2ec-0b842336dafc",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a7bd45-3634-4ef0-9ea6-38c2b9c97c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "horizons = [36, 48 ,60, 72, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3daf8141-c74c-4a94-8c48-d3a39cf41cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data():\n",
    "    \"\"\"\n",
    "    Load data, normalize and get data splits\n",
    "    \"\"\"\n",
    "    data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_t_2m.feather\")\n",
    "    #data = data[data[\"fcst_hour\"].isin(horizons)]\n",
    "    #Dropna\n",
    "    data.dropna(inplace=True)\n",
    "    data_np = data.iloc[:,3:-2].drop(\"obs_tm\", axis = 1).to_numpy()\n",
    "    #Create label encoding for embedding\n",
    "    label_enc = LabelEncoder()\n",
    "    encoding = label_enc.fit_transform(data_np[:,0])\n",
    "    data_np[:,0] = encoding\n",
    "    \n",
    "    Y = data_np[:,1]\n",
    "    X = np.delete(data_np, 1, axis = 1)\n",
    "    train_val_data_X, test_data_X, train_val_data_Y, test_data_Y = train_test_split(X,Y, test_size = 0.1)\n",
    "    train_data_X, val_data_X, train_data_Y,val_data_Y = train_test_split(train_val_data_X,train_val_data_Y, test_size = 0.2)\n",
    "\n",
    "    #Normalize features data based on train set\n",
    "    feature_scaler = Normalizer()\n",
    "    emb = train_data_X[:,0]\n",
    "    train_data_X = feature_scaler.fit_transform(train_data_X)\n",
    "    train_data_X[:,0] = emb\n",
    "    \n",
    "    emb = val_data_X[:,0]\n",
    "    val_data_X = feature_scaler.transform(val_data_X)\n",
    "    val_data_X[:,0] = emb\n",
    "    \n",
    "    emb = test_data_X[:,0]\n",
    "    test_data_X = feature_scaler.transform(test_data_X)\n",
    "    test_data_X[:,0] = emb\n",
    "    \n",
    "\n",
    "    #Normalize target and save retransform\n",
    "    target_scaler = StandardScaler()\n",
    "    train_data_Y = target_scaler.fit_transform(train_data_Y.reshape(-1,1))\n",
    "    val_data_Y = target_scaler.transform(val_data_Y.reshape(-1,1))\n",
    "    \n",
    "    return train_data_X, train_data_Y, val_data_X, val_data_Y, test_data_X, test_data_Y, feature_scaler, target_scaler, label_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6c0048-357c-4145-9a03-f78d32d81a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, val_X, val_Y, test_X, test_Y, feature_scaler, target_scaler, label_encoder = get_split_data()\n",
    "no_features = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe6315-fa3a-4000-a2de-7c7714564f6e",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c401b24-2bb3-4032-8387-68d8759625eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crps_cost_function(y_true, y_pred):\n",
    "    \"\"\"Compute the CRPS cost function for a normal distribution defined by\n",
    "    the mean and standard deviation.\n",
    "    Code inspired by Kai Polsterer (HITS).\n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Tensor containing predictions: [mean, std]\n",
    "    Returns:\n",
    "        mean_crps: Scalar with mean CRPS over batch    \"\"\"\n",
    "\n",
    "    # Split input\n",
    "    mu = y_pred[:, 0]\n",
    "    sigma = y_pred[:, 1]\n",
    "    y_true = y_true[:, 0]   # Need to also get rid of axis 1 to match!\n",
    "\n",
    "    # To stop sigma from becoming negative we first have to \n",
    "    # convert it the the variance and then take the square\n",
    "    # root again. \n",
    "    var = K.square(sigma)\n",
    "    # The following three variables are just for convenience\n",
    "    loc = (y_true - mu) / K.sqrt(var)\n",
    "    phi = 1.0 / np.sqrt(2.0 * np.pi) * K.exp(-K.square(loc) / 2.0)\n",
    "    Phi = 0.5 * (1.0 + erf(loc / np.sqrt(2.0)))\n",
    "    # First we will compute the crps for each input/target pair\n",
    "    crps =  K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
    "    # Then we take the mean. The cost is now a scalar\n",
    "    return K.mean(crps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650ec674-e165-43a9-b763-fe6b866ac6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model(train_X, train_Y, no_features, n_embeddings = 65, no_outputs = 2):\n",
    "    \"\"\"\n",
    "    trainX -- input values; shape: [number of samples, no_features]\n",
    "    trainY -- output values; shape: [number of samples, 2\n",
    "    \"\"\"    \n",
    "    inp = Input(shape = no_features+1)\n",
    "    #Extract embedding features\n",
    "    horizon = inp[:,0]\n",
    "    features = inp[:,1:]\n",
    "    \n",
    "    #Embedding layer\n",
    "    horizon_emb = Embedding(input_dim = n_embeddings, output_dim = 4)(horizon)\n",
    "    \n",
    "    #Concatenate\n",
    "    conc = Concatenate(axis = 1)([features,horizon_emb])\n",
    "    \n",
    "    #Hidden layer\n",
    "    #hidden = Dense(30, activation = \"relu\")(conc)\n",
    "    \n",
    "    #Linear layer\n",
    "    outputs = Dense(no_outputs, activation = \"linear\")(conc)\n",
    "    model = Model(inputs = inp, outputs = outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b60d1f33-79ac-4881-8b89-c7afb299c20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 41)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None,)              0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (None, 40)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 4)            260         tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 44)           0           tf.__operators__.getitem_1[0][0] \n",
      "                                                                 embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            90          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 350\n",
      "Trainable params: 350\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = basic_model(train_X, train_Y, no_features)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4835ea46-5637-496d-97a5-e22b4be0c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_X, train_Y, val_X, val_Y, no_features, batch_size, epochs, learning_rate):\n",
    "    #Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    #Early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5, min_delta = 1e-5)\n",
    "    #Compile model\n",
    "    model.compile(optimizer = optimizer, loss = crps_cost_function)\n",
    "    model.fit(train_X, train_Y, validation_data = (val_X, val_Y), batch_size = batch_size, epochs = EPOCHS, shuffle = True, callbacks = [callback], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d5693de-5bd9-4505-8f77-d53e444e144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 30\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd58e03d-28a3-417e-90e9-55302e6b1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.3664 - val_loss: 0.3689\n",
      "Epoch 2/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3650 - val_loss: 0.3688\n",
      "Epoch 3/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3668 - val_loss: 0.3690\n",
      "Epoch 4/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3693 - val_loss: 0.3686\n",
      "Epoch 5/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3658 - val_loss: 0.3682\n",
      "Epoch 6/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3645 - val_loss: 0.3689\n",
      "Epoch 7/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3657 - val_loss: 0.3679\n",
      "Epoch 8/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3641 - val_loss: 0.3680\n",
      "Epoch 9/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3660 - val_loss: 0.3683\n",
      "Epoch 10/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3627 - val_loss: 0.3690\n",
      "Epoch 11/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3644 - val_loss: 0.3685\n",
      "Epoch 12/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3650 - val_loss: 0.3677\n",
      "Epoch 13/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3642 - val_loss: 0.3680\n",
      "Epoch 14/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3656 - val_loss: 0.3680\n",
      "Epoch 15/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3644 - val_loss: 0.3677\n",
      "Epoch 16/30\n",
      "91/91 [==============================] - 0s 5ms/step - loss: 0.3659 - val_loss: 0.3677\n",
      "Epoch 17/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3665 - val_loss: 0.3676\n",
      "Epoch 18/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3659 - val_loss: 0.3675\n",
      "Epoch 19/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3643 - val_loss: 0.3675\n",
      "Epoch 20/30\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 0.3648 - val_loss: 0.3677\n",
      "Epoch 21/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3618 - val_loss: 0.3674\n",
      "Epoch 22/30\n",
      "91/91 [==============================] - 0s 6ms/step - loss: 0.3635 - val_loss: 0.3677\n",
      "Epoch 23/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3628 - val_loss: 0.3678\n",
      "Epoch 24/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3646 - val_loss: 0.3673\n",
      "Epoch 25/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3656 - val_loss: 0.3672\n",
      "Epoch 26/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3631 - val_loss: 0.3675\n",
      "Epoch 27/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3641 - val_loss: 0.3669\n",
      "Epoch 28/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3642 - val_loss: 0.3678\n",
      "Epoch 29/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3653 - val_loss: 0.3677\n",
      "Epoch 30/30\n",
      "91/91 [==============================] - 1s 6ms/step - loss: 0.3648 - val_loss: 0.3676\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_X, train_Y, val_X, val_Y, no_features, BATCH_SIZE, EPOCHS, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77040432-055d-4329-b3c5-832009a895bc",
   "metadata": {},
   "source": [
    "# Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "724836a2-5f92-4664-b412-43891791b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get prediction\n",
    "pred = model.predict(test_X)\n",
    "#Retransform\n",
    "pred = target_scaler.inverse_transform(pred)\n",
    "#Square and root results\n",
    "pred[:,1] = np.sqrt(pred[:,1]**2)\n",
    "#Convert prediction to quantiles\n",
    "quantile_pred = np.zeros(shape = (pred.shape[0],5))\n",
    "for cnt,x in enumerate(pred):\n",
    "    quantile_pred[cnt] = norm.ppf(quantiles, loc = x[0], scale = x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24ba71-ee97-4905-a7d1-d7b12b7e6afc",
   "metadata": {},
   "source": [
    "## Evaluate data on realizations with pinball loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ed30c23-c19a-4ab9-a2e4-886ae8a7a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinball loss for quantile 0.025 : \t 0.37988720622299943\n",
      "Pinball loss for quantile 0.25 : \t 1.807253495961178\n",
      "Pinball loss for quantile 0.5 : \t 2.1711784838089847\n",
      "Pinball loss for quantile 0.75 : \t 2.1022124017870065\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pinball_loss at 0x0000020CBD3DA4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Pinball loss for quantile 0.975 : \t 0.6456338901166231\n"
     ]
    }
   ],
   "source": [
    "for cnt,quantile in enumerate(quantiles):\n",
    "    loss = pinball_loss(np.squeeze(test_Y), quantile_pred[:,cnt], tau = quantile).numpy()\n",
    "    print(\"Pinball loss for quantile {} : \\t {}\".format(quantile,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06d9e6-501e-4dc9-9774-e2eeb9f3df0d",
   "metadata": {},
   "source": [
    "## Evaluate naive forecast on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c292058-fa01-498e-947c-513092bb5be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinball loss for quantile 0.025 : \t 0.4066453913797322\n",
      "Pinball loss for quantile 0.25 : \t 3.0042401295402374\n",
      "Pinball loss for quantile 0.5 : \t 5.885685021777776\n",
      "Pinball loss for quantile 0.75 : \t 8.76281391038621\n",
      "Pinball loss for quantile 0.975 : \t 11.343022199458263\n"
     ]
    }
   ],
   "source": [
    "naive_pred = np.quantile(test_X[:,1:], quantiles, axis = 1)\n",
    "for cnt,quantile in enumerate(quantiles):\n",
    "    loss = pinball_loss(np.squeeze(test_Y), naive_pred[cnt], tau = quantile).numpy()\n",
    "    print(\"Pinball loss for quantile {} : \\t {}\".format(quantile,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa53c3-7838-4d56-b1df-d3bc626bdd52",
   "metadata": {},
   "source": [
    "# Predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d1fd937-e836-484f-9908-e63656c5de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_data(name):\n",
    "    if name == \"temperature\":\n",
    "        method = \"t_2m\"\n",
    "    elif name == \"wind\":\n",
    "        method = \"wind_mean_10m\"\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        return None\n",
    "    #Set current date\n",
    "    current_date = date.today().strftime(\"%Y%m%d\")\n",
    "    path = \"data/berlin_data/icon_data/icon-eu-eps_{}00_{}_Berlin.txt\".format(current_date, method)\n",
    "    new_data = pd.read_csv(path.format(current_date.replace(\"-\",\"\")), skiprows = 3, sep = \"|\").dropna(axis = 1)\n",
    "    new_data.columns = new_data.columns.str.replace(\" \", \"\")\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "661acb48-8b63-46ac-9ae6-a3fe78746d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_forecast(name, horizons, feature_scaler, label_encoder, model, save = False):\n",
    "    if name == \"temperature\":\n",
    "        method = \"t_2m\"\n",
    "    elif name == \"wind\":\n",
    "        method = \"wind_mean_10m\"\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        return None\n",
    "    #Get data\n",
    "    data = get_pred_data(name)\n",
    "    data = data[data[\"fcst_hour\"].isin(horizons)].to_numpy()\n",
    "    #Label encoding\n",
    "    encoding = label_encoder.transform(data[:,0])\n",
    "    #Normalize\n",
    "    data_pred = feature_scaler.transform(data)\n",
    "    data_pred[:,0] = encoding\n",
    "    #Predict\n",
    "    pred = model.predict(data_pred)\n",
    "    pred = target_scaler.inverse_transform(pred)\n",
    "    \n",
    "    #Create final prediction dataframe\n",
    "    final_prediction = pd.DataFrame(columns = [\"forecast_date\",\"target\",\"horizon\",\"q0.025\",\"q0.25\",\"q0.5\",\"q0.75\",\"q0.975\"], index = np.arange(0,5))\n",
    "    final_prediction[\"forecast_date\"] = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    final_prediction[\"horizon\"] = [\"{} hour\".format(x) for x in horizons]\n",
    "    final_prediction[\"target\"] = name\n",
    "    \n",
    "    #Save prediction to dataframe\n",
    "    for cnt,x in enumerate(pred):\n",
    "        final_prediction.loc[final_prediction[\"horizon\"] == \"{} hour\".format(horizons[cnt]), final_prediction.columns[3:]] = (norm.ppf(quantiles, loc = x[0], scale = x[1]))\n",
    "        \n",
    "    #Save prediction\n",
    "    if save == True:\n",
    "        final_prediction.to_pickle(\"../evaluation/predictions/single/{}_{}\".format(name, date.today().strftime(\"%Y-%m-%d\")))\n",
    "    \n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f21ff9e-c155-44cb-83a4-cf729a688a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_final_forecast(\"temperature\",horizons, feature_scaler, label_encoder, model, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71987b24-0f29-4946-8126-d16e3ef7893d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
