{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95c87b87-6742-42a5-a360-caf0067f0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler, LabelEncoder\n",
    "from tensorflow_addons.losses import pinball_loss\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from scipy import stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38a61ef2-6979-4c75-b699-8ab25c8b97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "horizons = [36, 48 ,60, 72, 84]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b058c-cc23-4ea4-8879-2fa0d4454cc4",
   "metadata": {},
   "source": [
    "# Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33d46b04-690f-4c5e-9826-865a90030282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>init_tm</th>\n",
       "      <th>met_var</th>\n",
       "      <th>location</th>\n",
       "      <th>fcst_hour</th>\n",
       "      <th>obs_tm</th>\n",
       "      <th>obs</th>\n",
       "      <th>ens_1</th>\n",
       "      <th>ens_2</th>\n",
       "      <th>ens_3</th>\n",
       "      <th>ens_4</th>\n",
       "      <th>...</th>\n",
       "      <th>ens_36</th>\n",
       "      <th>ens_37</th>\n",
       "      <th>ens_38</th>\n",
       "      <th>ens_39</th>\n",
       "      <th>ens_40</th>\n",
       "      <th>ens_mean</th>\n",
       "      <th>ens_var</th>\n",
       "      <th>mean_pressure</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>vmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2018-12-19 00:00:00+00:00</td>\n",
       "      <td>wind_10m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-12-19 00:00:00+00:00</td>\n",
       "      <td>12.60</td>\n",
       "      <td>9.69</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.85</td>\n",
       "      <td>9.80</td>\n",
       "      <td>...</td>\n",
       "      <td>10.53</td>\n",
       "      <td>11.33</td>\n",
       "      <td>9.40</td>\n",
       "      <td>9.62</td>\n",
       "      <td>9.97</td>\n",
       "      <td>9.71750</td>\n",
       "      <td>1.179127</td>\n",
       "      <td>1022.41475</td>\n",
       "      <td>3.26025</td>\n",
       "      <td>23.36562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2018-12-19 00:00:00+00:00</td>\n",
       "      <td>wind_10m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-12-19 01:00:00+00:00</td>\n",
       "      <td>12.60</td>\n",
       "      <td>10.97</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.26</td>\n",
       "      <td>10.12</td>\n",
       "      <td>...</td>\n",
       "      <td>11.43</td>\n",
       "      <td>11.85</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.76</td>\n",
       "      <td>9.39</td>\n",
       "      <td>10.29675</td>\n",
       "      <td>1.028694</td>\n",
       "      <td>1021.70900</td>\n",
       "      <td>0.58050</td>\n",
       "      <td>23.36562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2018-12-19 00:00:00+00:00</td>\n",
       "      <td>wind_10m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-12-19 02:00:00+00:00</td>\n",
       "      <td>12.24</td>\n",
       "      <td>11.76</td>\n",
       "      <td>11.47</td>\n",
       "      <td>10.54</td>\n",
       "      <td>10.51</td>\n",
       "      <td>...</td>\n",
       "      <td>11.90</td>\n",
       "      <td>12.27</td>\n",
       "      <td>10.36</td>\n",
       "      <td>11.58</td>\n",
       "      <td>9.67</td>\n",
       "      <td>10.99725</td>\n",
       "      <td>0.896077</td>\n",
       "      <td>1020.92025</td>\n",
       "      <td>0.96325</td>\n",
       "      <td>23.36562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2018-12-19 00:00:00+00:00</td>\n",
       "      <td>wind_10m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-12-19 03:00:00+00:00</td>\n",
       "      <td>11.52</td>\n",
       "      <td>12.16</td>\n",
       "      <td>12.04</td>\n",
       "      <td>10.95</td>\n",
       "      <td>11.47</td>\n",
       "      <td>...</td>\n",
       "      <td>12.23</td>\n",
       "      <td>12.78</td>\n",
       "      <td>10.41</td>\n",
       "      <td>11.60</td>\n",
       "      <td>10.23</td>\n",
       "      <td>11.40975</td>\n",
       "      <td>0.622141</td>\n",
       "      <td>1020.62650</td>\n",
       "      <td>12.33875</td>\n",
       "      <td>23.36562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2018-12-19 00:00:00+00:00</td>\n",
       "      <td>wind_10m</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-12-19 04:00:00+00:00</td>\n",
       "      <td>10.08</td>\n",
       "      <td>12.57</td>\n",
       "      <td>12.79</td>\n",
       "      <td>11.21</td>\n",
       "      <td>12.36</td>\n",
       "      <td>...</td>\n",
       "      <td>12.91</td>\n",
       "      <td>13.52</td>\n",
       "      <td>11.14</td>\n",
       "      <td>12.16</td>\n",
       "      <td>11.34</td>\n",
       "      <td>12.02400</td>\n",
       "      <td>0.558978</td>\n",
       "      <td>1020.63250</td>\n",
       "      <td>32.37400</td>\n",
       "      <td>23.36562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     init_tm   met_var location  fcst_hour  \\\n",
       "65 2018-12-19 00:00:00+00:00  wind_10m   Berlin        0.0   \n",
       "66 2018-12-19 00:00:00+00:00  wind_10m   Berlin        1.0   \n",
       "67 2018-12-19 00:00:00+00:00  wind_10m   Berlin        2.0   \n",
       "68 2018-12-19 00:00:00+00:00  wind_10m   Berlin        3.0   \n",
       "69 2018-12-19 00:00:00+00:00  wind_10m   Berlin        4.0   \n",
       "\n",
       "                      obs_tm    obs  ens_1  ens_2  ens_3  ens_4  ...  ens_36  \\\n",
       "65 2018-12-19 00:00:00+00:00  12.60   9.69   9.60   9.85   9.80  ...   10.53   \n",
       "66 2018-12-19 01:00:00+00:00  12.60  10.97  10.49  10.26  10.12  ...   11.43   \n",
       "67 2018-12-19 02:00:00+00:00  12.24  11.76  11.47  10.54  10.51  ...   11.90   \n",
       "68 2018-12-19 03:00:00+00:00  11.52  12.16  12.04  10.95  11.47  ...   12.23   \n",
       "69 2018-12-19 04:00:00+00:00  10.08  12.57  12.79  11.21  12.36  ...   12.91   \n",
       "\n",
       "    ens_37  ens_38  ens_39  ens_40  ens_mean   ens_var  mean_pressure  \\\n",
       "65   11.33    9.40    9.62    9.97   9.71750  1.179127     1022.41475   \n",
       "66   11.85   10.08   10.76    9.39  10.29675  1.028694     1021.70900   \n",
       "67   12.27   10.36   11.58    9.67  10.99725  0.896077     1020.92025   \n",
       "68   12.78   10.41   11.60   10.23  11.40975  0.622141     1020.62650   \n",
       "69   13.52   11.14   12.16   11.34  12.02400  0.558978     1020.63250   \n",
       "\n",
       "    cloud_coverage      vmax  \n",
       "65         3.26025  23.36562  \n",
       "66         0.58050  23.36562  \n",
       "67         0.96325  23.36562  \n",
       "68        12.33875  23.36562  \n",
       "69        32.37400  23.36562  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wind data\n",
    "wind_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_wind_10m.feather\")\n",
    "#Pressure data\n",
    "pressure_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_mslp.feather\")\n",
    "pressure_data.rename({\"ens_mean\":\"mean_pressure\"}, axis = 1, inplace = True)\n",
    "#Cloud data\n",
    "cloud_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_clct.feather\")\n",
    "cloud_data.rename({\"ens_mean\":\"cloud_coverage\"}, axis = 1, inplace = True)\n",
    "#Vmax data\n",
    "max_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_vmax_10m.feather\")\n",
    "max_data.rename({\"ens_mean\":\"vmax\"}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "data = wind_data.merge(pressure_data[[\"init_tm\",\"fcst_hour\",\"mean_pressure\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(cloud_data[[\"init_tm\",\"fcst_hour\",\"cloud_coverage\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(max_data[[\"init_tm\",\"fcst_hour\",\"vmax\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "#Replace vmax NaNs by mean\n",
    "vmax_mean = data[\"vmax\"].mean()\n",
    "data.loc[:,\"vmax\"].fillna(vmax_mean, inplace = True)\n",
    "data.dropna(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b5481-eb7e-49ca-afec-58682f55ec56",
   "metadata": {},
   "source": [
    "## Create positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f126faa5-7653-40c0-8747-891f31410285",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = pd.DataFrame(index=pd.DatetimeIndex(data[\"obs_tm\"]))\n",
    "pos_enc[\"Dayofyear\"] = pos_enc.index.dayofyear\n",
    "pos_enc[\"n_days\"] = 365\n",
    "pos_enc.loc[pos_enc.index.year==2020,\"n_days\"] = 366\n",
    "#Calculate actual positional encoding\n",
    "cos_encoding = np.cos(2*math.pi*pos_enc[\"Dayofyear\"]/pos_enc[\"n_days\"])\n",
    "data[\"pos_enc_1\"] = cos_encoding.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b102af7-0f3f-4a64-a4a9-5ca473ece0e7",
   "metadata": {},
   "source": [
    "## Train, val, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e235d099-c210-4ffd-8f05-ded00a446025",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataframe, test_dataframe = train_test_split(data, test_size = 0.2)#, random_state = 1)\n",
    "train_dataframe, val_dataframe = train_test_split(data, test_size = 0.2)#, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7426d-03b3-45db-89db-c30902fb7655",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b72e22d-2362-4c63-a868-8c373b12890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataframe, label_encoder = None,feature_scaler = None, target_scaler = None, learn = False):\n",
    "    #Drop unused columns\n",
    "    data = dataframe.copy()\n",
    "    data.drop([\"init_tm\", \"met_var\", \"location\",  \"ens_var\", \"obs_tm\"], axis = 1, inplace = True)\n",
    "    data = data.to_numpy()\n",
    "    if learn == True:\n",
    "        label_encoder = LabelEncoder()\n",
    "        feature_scaler = StandardScaler()\n",
    "        target_scaler = StandardScaler()\n",
    "        #Learn label encoding for horizons\n",
    "        label = label_encoder.fit_transform(data[:,0])\n",
    "        #Learn target scaling\n",
    "        target_scaled = target_scaler.fit_transform(data[:,1].reshape(-1,1))\n",
    "        #Learn feature scaling\n",
    "        feature_scaled = feature_scaler.fit_transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data, label_encoder, feature_scaler, target_scaler\n",
    "    \n",
    "    else:\n",
    "        #Learn labels\n",
    "        label = label_encoder.transform(data[:,0])\n",
    "        #Scale target\n",
    "        target_scaled = target_scaler.transform(data[:,1].reshape(-1,1))\n",
    "        #Scale features\n",
    "        feature_scaled = feature_scaler.transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40e886bd-d209-4638-b8dc-2200bf8d96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, label_encoder, feature_scaler, target_scaler = normalize(train_val_dataframe, learn = True)\n",
    "train= normalize(train_dataframe, label_encoder, feature_scaler, target_scaler)\n",
    "test = normalize(test_dataframe, label_encoder, feature_scaler, target_scaler)\n",
    "val = normalize(val_dataframe, label_encoder, feature_scaler, target_scaler)\n",
    "#Number of encodings\n",
    "n_encodings = len(np.unique(train[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba78957c-9346-45cf-bfe3-6fd950b6e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(input_data, predict = False):\n",
    "    #Extract forecast embedding\n",
    "    horizon_emb = input_data[:,0]\n",
    "    \n",
    "    if predict == False:        \n",
    "        #Extract features\n",
    "        features = input_data[:,2:]\n",
    "        # Extract target\n",
    "        target = np.expand_dims(input_data[:,1],1)\n",
    "        return [features, horizon_emb], target\n",
    "    else:\n",
    "        #Extract features\n",
    "        features = input_data[:,1:]\n",
    "        return [features, horizon_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f64c0313-aaba-4927-b7e0-c028ef2871de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target = convert_format(train)\n",
    "val_data, val_target = convert_format(val)\n",
    "test_data, test_target = convert_format(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f9a31-842f-4af8-9e5e-65938d9aa29c",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "427ab472-e25f-4453-85e6-b2b6d58ced77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model(tf.keras.Model):    \n",
    "    def __init__(self, n_embeddings = n_encodings):\n",
    "        super(base_model, self).__init__()\n",
    "        #Embedding layers\n",
    "        self.embedding = Embedding(input_dim = n_embeddings, output_dim = 4)\n",
    "        #Create Dense layers\n",
    "        self.hidden = Dense(64, activation = \"relu\")\n",
    "        self.hidden2 = Dense(32, activation = \"relu\")\n",
    "        self.out = Dense(5, activation = \"linear\")\n",
    "\n",
    "    def call(self, input_data):\n",
    "        #Extract data\n",
    "        features, horizon_emb = input_data\n",
    "        #Calculate embedding\n",
    "        emb = self.embedding(horizon_emb)\n",
    "        emb = tf.squeeze(emb, axis = 1)\n",
    "        conc = Concatenate(axis = 1)([features, emb])\n",
    "        #Calculate output\n",
    "        output = self.hidden(conc)\n",
    "        output = self.hidden2(output)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6330b732-4945-4fee-928d-68ce05bc9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e37fbb5c-6566-4d15-a922-01776d1353ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "import tensorflow as tf\n",
    "from typeguard import typechecked\n",
    "from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
    "from tensorflow_addons.utils.types import TensorLike, FloatTensorLike\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def smooth_pinball_loss(\n",
    "    y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5,\n",
    "    alpha: FloatTensorLike = 0.001\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes the pinball loss between `y_true` and `y_pred`.\n",
    "    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n",
    "    In the context of regression this loss yields an estimator of the tau\n",
    "    conditional quantile.\n",
    "    See: https://en.wikipedia.org/wiki/Quantile_regression\n",
    "    Usage:\n",
    "    >>> loss = tfa.losses.pinball_loss([0., 0., 1., 1.],\n",
    "    ... [1., 1., 1., 0.], tau=.1)\n",
    "    >>> loss\n",
    "    <tf.Tensor: shape=(), dtype=float32, numpy=0.475>\n",
    "    Args:\n",
    "      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`\n",
    "      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
    "      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n",
    "        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n",
    "        the context of quantile regression, the value of tau determines the\n",
    "        conditional quantile level. When tau = 0.5, this amounts to l1\n",
    "        regression, an estimator of the conditional median (0.5 quantile).\n",
    "    Returns:\n",
    "        pinball_loss: 1-D float `Tensor` with shape [batch_size].\n",
    "    References:\n",
    "      - https://en.wikipedia.org/wiki/Quantile_regression\n",
    "      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n",
    "    \"\"\"\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "\n",
    "    delta_y = y_true - y_pred\n",
    "    delta_y = delta_y\n",
    "    #Implement smooth loss\n",
    "    pinball = tau * delta_y + alpha * tf.math.softplus(-delta_y/alpha)\n",
    "    return tf.reduce_mean(pinball, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e41247b-7095-4db1-bf39-568a37fa6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, train_target, validation_data, batch_size, epochs, learning_rate, fine_tuning = True):\n",
    "    model = base_model()    \n",
    "    #Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    #Callbacks\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "    model.compile(optimizer = optimizer, loss = lambda true,pred: pinball_loss(true, pred, tau = quantiles))\n",
    "    #model.compile(optimizer = optimizer, loss = lambda true,pred: smooth_pinball_loss(true, pred, tau = quantiles))\n",
    "    #Normal fit\n",
    "    history1 = model.fit(x = train_data, y = train_target, validation_data = validation_data, epochs = epochs, batch_size = BATCH_SIZE, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    \n",
    "    #Fine tuning\n",
    "    if fine_tuning == True:\n",
    "        enc_horizons = label_encoder.transform(horizons)\n",
    "        train_filtering = np.isin(train_data[1], enc_horizons)\n",
    "        train_data_fine = [train_data[0][train_filtering], train_data[1][train_filtering]]\n",
    "        train_target_fine = train_target[train_filtering]\n",
    "        #Val filtering\n",
    "        val_data, val_target = validation_data\n",
    "        val_filtering = np.isin(val_data[1], enc_horizons)\n",
    "        val_data_fine = [val_data[0][val_filtering], val_data[1][val_filtering]]\n",
    "        val_target_fine = val_target[val_filtering]\n",
    "        validation_data_fine = (val_data_fine, val_target_fine)\n",
    "        \n",
    "        #New optimizer\n",
    "        history2 = model.fit(x = train_data_fine, y = train_target_fine, validation_data = validation_data_fine, epochs = epochs, batch_size = 256, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    return model, [history1, history2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b654dc1-0fad-4efa-a1b2-7555e6a901e1",
   "metadata": {},
   "source": [
    "# Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d8a8594-3ca3-4a63-8bd3-5b54568f0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_training(train_data, train_target, validation_data, test_data,  BATCH_SIZE, EPOCHS, learning_rate, n = 10):\n",
    "    predictions = np.zeros(shape = (len(test_data[0]),5))\n",
    "    for i in range(n):\n",
    "        model,_ = train_model(train_data, train_target, validation_data, BATCH_SIZE, EPOCHS, learning_rate)\n",
    "        pred = model.predict(test_data)\n",
    "        predictions += pred\n",
    "        print(\"Finished Training {}\".format(i+1))\n",
    "    predictions = predictions/n\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9f4c25b-7d16-4082-982b-b149e3026feb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_55604/529686914.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_55604/3453098263.py\u001b[0m in \u001b[0;36maggregate_training\u001b[1;34m(train_data, train_target, validation_data, test_data, BATCH_SIZE, EPOCHS, learning_rate, n)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_55604/514705764.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(train_data, train_target, validation_data, batch_size, epochs, learning_rate, fine_tuning)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#model.compile(optimizer = optimizer, loss = lambda true,pred: smooth_pinball_loss(true, pred, tau = quantiles))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#Normal fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mhistory1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#Fine tuning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = aggregate_training(train_data, train_target, (val_data,val_target), test_data, BATCH_SIZE, EPOCHS, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e9db2-9a20-48f2-a977-e6ccff02f431",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pinball Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d8f98-b230-4d0f-951e-49095c770aab",
   "metadata": {},
   "source": [
    "### All horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d0669-31d8-4edc-be3a-d3cce8bae007",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "for cnt,quantile in enumerate(quantiles):\n",
    "    loss = mean_pinball_loss(test_target.reshape(-1), predictions[:,cnt].reshape(-1), alpha = quantile)\n",
    "    total_loss += loss\n",
    "    print(\"Pinball loss for quantile {} : \\t {}\".format(quantile,loss))\n",
    "print(\"Pinball Loss total: {}\".format(total_loss/len(quantiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7d0da-aae3-4f23-84ef-ef7349ee69fb",
   "metadata": {},
   "source": [
    "### Specific horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3887da4-dcd0-4f3a-be2b-5551a8ec1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = test_dataframe[[\"fcst_hour\",\"obs\"]].copy()\n",
    "eval_df[\"obs\"] = target_scaler.transform(eval_df[\"obs\"].to_numpy().reshape(-1,1))\n",
    "for cnt,quantile in enumerate(quantiles):\n",
    "    eval_df[quantile] = predictions[:,cnt]\n",
    "eval_df = eval_df[eval_df[\"fcst_hour\"].isin(horizons)]\n",
    "\n",
    "total_loss = 0\n",
    "for cnt,quantile in enumerate(quantiles):\n",
    "    loss = mean_pinball_loss(eval_df[\"obs\"], eval_df[quantile], alpha = quantile)\n",
    "    total_loss += loss\n",
    "    print(\"Pinball loss for quantile {} : \\t {}\".format(quantile,loss))\n",
    "print(\"Pinball Loss total: {}\".format(total_loss/len(quantiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c654a5-218b-4b82-96ba-8fd96fa578b6",
   "metadata": {},
   "source": [
    "## Plausability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34baf7f8-09fd-4212-9f11-4d17279e16e0",
   "metadata": {},
   "source": [
    "### All horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40555fc2-d96c-4a92-a0d5-1f87e42470f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt,quantile in enumerate(quantiles):\n",
    "    q_smaller = (predictions[:,cnt] > test_target.flatten()).sum()\n",
    "    emp_quant = q_smaller / predictions[:,cnt].size\n",
    "    print(\"Quantile met for quantile = {}: \\t {} %\".format(quantile, np.round(emp_quant,4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc74d5-b73f-453f-849f-cb63bb06b558",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specific horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828a49d-b445-476c-ac43-45e0d941e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "for quantile in quantiles:\n",
    "    q_smaller = (eval_df[quantile] > eval_df[\"obs\"]).sum()\n",
    "    emp_quant = q_smaller / eval_df[quantile].size\n",
    "    print(\"Quantile met for quantile = {}: \\t {} %\".format(quantile, np.round(emp_quant,4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8045cc-6b4e-4094-b50a-e9c42bd23775",
   "metadata": {},
   "source": [
    "## Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4d7de-6109-42f4-b212-2dc05c56bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plotting dataframe\n",
    "data_plot = test_dataframe[[\"obs_tm\",\"obs\",\"fcst_hour\"]].copy()\n",
    "for cnt, quantile in enumerate(quantiles):\n",
    "    data_plot[\"q{}\".format(quantile)] = target_scaler.inverse_transform(predictions[:,cnt].reshape(-1,1)).reshape(-1)\n",
    "\n",
    "#Extract horizon\n",
    "h=48\n",
    "data_plot = data_plot[data_plot[\"fcst_hour\"]==h]\n",
    "data_plot.sort_values(by = \"obs_tm\", inplace = True)\n",
    "\n",
    "fig, axs = plt.subplots(figsize = (20,10))\n",
    "sns.lineplot(x = \"obs_tm\", y = \"obs\", data = data_plot, label = \"True value\")\n",
    "sns.lineplot(x = \"obs_tm\", y = \"q0.5\", data = data_plot, label = \"50% quantile\")\n",
    "sns.lineplot(x = \"obs_tm\", y = \"q0.025\", data = data_plot, color = \"blue\", label = \"95% interval\", alpha = 0.5)\n",
    "sns.lineplot(x = \"obs_tm\", y = \"q0.975\", data = data_plot, color = \"blue\", alpha = 0.5)\n",
    "axs.fill_between(x = \"obs_tm\", y1 = \"q0.025\", y2 = \"q0.975\", data = data_plot, alpha = 0.1, color = \"blue\")\n",
    "\n",
    "sns.lineplot(x = \"obs_tm\", y = \"q0.25\", data = data_plot, color = \"green\", label = \"50% interval\", alpha = 0.5)\n",
    "sns.lineplot(x = \"obs_tm\", y = \"q0.75\", data = data_plot, color = \"green\", alpha = 0.5)\n",
    "axs.fill_between(x = \"obs_tm\", y1 = \"q0.25\", y2 =\"q0.75\", data = data_plot, alpha = 0.1, color = \"green\")\n",
    "axs.set_title(\"Plot for horizon: h = {}\".format(h),size = 17)\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c582add-e0fe-4582-985b-cff49ee484c6",
   "metadata": {},
   "source": [
    "## Analyze crossing of quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92542a-41c7-4125-99c5-18c99569bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group prediction\n",
    "perc_wrong_total = np.sum(np.diff(predictions) < 0) / len(predictions) * 100\n",
    "\n",
    "#Single prediction\n",
    "model,_ = train_model(train_data, train_target, (val_data, val_target), BATCH_SIZE, EPOCHS, learning_rate)\n",
    "predictions_single = model.predict(test_data)\n",
    "#Get amount of wrongly specified quantiles\n",
    "perc_wrong_single = np.sum(np.diff(predictions_single) < 0) / len(predictions_single) * 100\n",
    "\n",
    "print(\"Amount of wrongly specified quantiles in single prediction: {:.4f}%\\nAmount of wrongly specified quantiles in aggregated prediction: {}%\".format(perc_wrong_single, perc_wrong_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d783c6-3f85-4823-ad89-88dfbd54e954",
   "metadata": {},
   "source": [
    "# Predict new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822308c-2d0b-4fb6-9874-f9dba32d4668",
   "metadata": {},
   "source": [
    "## Train on complete data without test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5d0d281-5cd3-43bc-97e3-1b3495246f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target = convert_format(train_val)\n",
    "val_data, val_target = convert_format(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e586b-40b4-47dd-8052-86281866f51a",
   "metadata": {},
   "source": [
    "## Predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c735098-8cf5-4b61-9973-ee5c1cf9301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = date.today().strftime(\"%Y%m%d\")\n",
    "path = \"data/berlin_data/icon_data/icon-eu-eps_{}00_wind_mean_10m_Berlin.txt\".format(current_date)\n",
    "new_data = pd.read_csv(path.format(current_date.replace(\"-\",\"\")), skiprows = 3, sep = \"|\").dropna(axis = 1)\n",
    "new_data.columns = new_data.columns.str.replace(\" \", \"\")\n",
    "new_data[\"ens_mean\"] = new_data.iloc[:,1:].mean(axis = 1)\n",
    "\n",
    "#Add pressure data\n",
    "pressure_pred = pd.read_csv(\"data/berlin_data/icon_data/icon-eu-eps_{}00_mslp_Berlin.txt\".format(current_date.replace(\"-\",\"\")), skiprows = 3, sep = \"|\").dropna(axis = 1)\n",
    "new_data[\"pressure_mean\"] = pressure_pred.iloc[:,1:].mean(axis = 1)\n",
    "\n",
    "#Add cloud data\n",
    "cloud_pred = pd.read_csv(\"data/berlin_data/icon_data/icon-eu-eps_{}00_clct_Berlin.txt\".format(current_date.replace(\"-\",\"\")), skiprows = 3, sep = \"|\").dropna(axis = 1)\n",
    "new_data[\"cloud_coverage\"] = cloud_pred.iloc[:,1:].mean(axis = 1)\n",
    "\n",
    "#Add vmax data\n",
    "max_pred = pd.read_csv(\"data/berlin_data/icon_data/icon-eu-eps_{}00_vmax_10m_Berlin.txt\".format(current_date.replace(\"-\",\"\")), skiprows = 3, sep = \"|\").dropna(axis = 1)\n",
    "new_data[\"vmax\"] = max_pred.iloc[:,1:].mean(axis = 1)\n",
    "\n",
    "#Filter horizons\n",
    "new_data = new_data[new_data[\"fcst_hour\"].isin(horizons)]\n",
    "\n",
    "#Create positional encoding\n",
    "date_list = [(date.today()+timedelta(x)) for x in horizons]\n",
    "new_data[\"day\"] = pd.DatetimeIndex(date_list).dayofyear\n",
    "new_data[\"pos_enc_1\"] = np.cos(2*math.pi*new_data[\"day\"]/365)\n",
    "new_data.drop(\"day\", axis = 1, inplace = True)\n",
    "# Normalize and get horizons\n",
    "new_data = new_data[new_data[\"fcst_hour\"].isin(horizons)].to_numpy()\n",
    "new_data[:,1:] = feature_scaler.transform(new_data[:,1:])\n",
    "new_data[:,0] = label_encoder.transform(new_data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15935e68-3d66-4718-891d-c07b28dddb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = convert_format(new_data, predict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbc89af2-ff03-4ee4-a1e0-62fbd5d7998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataframe\n",
    "final_prediction = pd.DataFrame(columns = [\"forecast_date\",\"target\",\"horizon\",\"q0.025\",\"q0.25\",\"q0.5\",\"q0.75\",\"q0.975\"], index = np.arange(0,5))\n",
    "final_prediction[\"forecast_date\"] = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "final_prediction[\"horizon\"] = [\"{} hour\".format(x) for x in horizons]\n",
    "final_prediction[\"target\"] = \"wind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cf67588-e806-4eec-b5c2-b5d40a2f9517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training 1\n",
      "Finished Training 2\n",
      "Finished Training 3\n",
      "WARNING:tensorflow:5 out of the last 404 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002524E8AEDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Finished Training 4\n",
      "WARNING:tensorflow:6 out of the last 405 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000252526C5700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Finished Training 5\n",
      "WARNING:tensorflow:7 out of the last 406 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000253F1C0D8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Finished Training 6\n",
      "WARNING:tensorflow:8 out of the last 407 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000252555389D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Finished Training 7\n",
      "WARNING:tensorflow:9 out of the last 408 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000253FE8290D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Finished Training 8\n",
      "WARNING:tensorflow:10 out of the last 409 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002541B8F4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Finished Training 9\n",
      "WARNING:tensorflow:11 out of the last 410 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000252526C50D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Finished Training 10\n"
     ]
    }
   ],
   "source": [
    "# Predict data\n",
    "predictions = aggregate_training(train_data, train_target, (val_data,val_target), pred_data, BATCH_SIZE, EPOCHS, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20deb63c-0af4-4dd2-9fa2-dead995c47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, quantile in enumerate(quantiles):\n",
    "    #Retransform predictions\n",
    "    final_pred = target_scaler.inverse_transform(predictions[:,cnt].reshape(-1,1))\n",
    "    final_prediction.loc[:,\"q{}\".format(quantile)] = final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a12ec3ef-a08b-4962-850f-5882ed486b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast_date</th>\n",
       "      <th>target</th>\n",
       "      <th>horizon</th>\n",
       "      <th>q0.025</th>\n",
       "      <th>q0.25</th>\n",
       "      <th>q0.5</th>\n",
       "      <th>q0.75</th>\n",
       "      <th>q0.975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>wind</td>\n",
       "      <td>36 hour</td>\n",
       "      <td>10.815668</td>\n",
       "      <td>14.738698</td>\n",
       "      <td>16.273245</td>\n",
       "      <td>17.592292</td>\n",
       "      <td>21.600636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>wind</td>\n",
       "      <td>48 hour</td>\n",
       "      <td>9.428716</td>\n",
       "      <td>13.955290</td>\n",
       "      <td>15.833398</td>\n",
       "      <td>17.641614</td>\n",
       "      <td>23.475375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>wind</td>\n",
       "      <td>60 hour</td>\n",
       "      <td>9.854185</td>\n",
       "      <td>13.222522</td>\n",
       "      <td>14.537260</td>\n",
       "      <td>16.036510</td>\n",
       "      <td>19.762636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>wind</td>\n",
       "      <td>72 hour</td>\n",
       "      <td>4.635388</td>\n",
       "      <td>8.998632</td>\n",
       "      <td>11.000033</td>\n",
       "      <td>13.198750</td>\n",
       "      <td>19.121748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>wind</td>\n",
       "      <td>84 hour</td>\n",
       "      <td>7.148840</td>\n",
       "      <td>12.375674</td>\n",
       "      <td>14.719133</td>\n",
       "      <td>16.793989</td>\n",
       "      <td>23.444304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  forecast_date target  horizon     q0.025      q0.25       q0.5      q0.75  \\\n",
       "0    2021-12-15   wind  36 hour  10.815668  14.738698  16.273245  17.592292   \n",
       "1    2021-12-15   wind  48 hour   9.428716  13.955290  15.833398  17.641614   \n",
       "2    2021-12-15   wind  60 hour   9.854185  13.222522  14.537260  16.036510   \n",
       "3    2021-12-15   wind  72 hour   4.635388   8.998632  11.000033  13.198750   \n",
       "4    2021-12-15   wind  84 hour   7.148840  12.375674  14.719133  16.793989   \n",
       "\n",
       "      q0.975  \n",
       "0  21.600636  \n",
       "1  23.475375  \n",
       "2  19.762636  \n",
       "3  19.121748  \n",
       "4  23.444304  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e40dc3ca-e8f4-4e7c-a32e-af2aa2cc998f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36    12.940976\n",
       "48    12.678293\n",
       "52    12.798293\n",
       "56     9.100732\n",
       "58    12.401463\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.read_csv(path.format(current_date.replace(\"-\",\"\")), skiprows = 3, sep = \"|\").dropna(axis = 1)\n",
    "new_data.columns = new_data.columns.str.replace(\" \", \"\")\n",
    "new_data[new_data[\"fcst_hour\"].isin(horizons)].quantile(0.5, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5cb1ee4-677a-4e33-9ec9-8b72640e3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction.to_pickle(\"../evaluation/predictions/single/{}_{}\".format(\"wind\", date.today().strftime(\"%Y-%m-%d\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce61106-a313-45c4-b486-961761a8b47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df8f23ee-dfe1-40fb-b8d7-f8f4a287d014",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e5931-bbf1-400d-a766-4a6b919a0ba9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check relationship with time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890b51a-6058-4de3-83cc-05b1825b0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3,figsize=(25,15))\n",
    "ax = ax.ravel()\n",
    "for cnt, quantile in enumerate(quantiles):\n",
    "    #Renormalize and calculate forecast error\n",
    "    error = mean_pinball_loss(test_dataframe[\"obs\"].to_numpy().reshape(1,-1), target_scaler.inverse_transform(predictions[cnt]).reshape(1,-1), multioutput=\"raw_values\", alpha=quantile)\n",
    "    test_dataframe[\"Forecast_error\"] = error\n",
    "    test_dataframe[\"day\"] = pd.DatetimeIndex(test_dataframe[\"obs_tm\"]).dayofyear\n",
    "    plot = test_dataframe.groupby(\"day\").mean().reset_index()\n",
    "\n",
    "    sns.regplot(x = \"day\", y = \"Forecast_error\", data=plot, ax = ax[cnt])\n",
    "    ax[cnt].set_xlabel(\"Day of year\",size = 15)\n",
    "    ax[cnt].set_ylabel(\"Forecast error\", size = 17)\n",
    "    ax[cnt].set_title(\"Correlation for q = {}\".format(quantile),size = 20)\n",
    "    r = stats.pearsonr(plot[\"day\"], plot[\"Forecast_error\"])[0]\n",
    "    rho = stats.spearmanr(plot[\"day\"], plot[\"Forecast_error\"])[0]\n",
    "    ax[cnt].annotate('Correlation:\\nr = {:.4f}\\n'.format(r)+r'$\\rho$ = {:.4f}'.format(rho),xy = (.01,.02), xycoords=ax[cnt].transAxes, size = 15)\n",
    "    \n",
    "#plt.savefig(\"wind_time_correlation_corrected_plot.pdf\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
