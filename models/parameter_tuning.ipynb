{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2684caa-a520-4b33-909a-fab7d08346a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from scipy import stats\n",
    "import math\n",
    "import optuna as opt\n",
    "from sklearn.model_selection import KFold\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57753f-8123-4463-8bc9-8e0707d6cb6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tune Wind model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8b7b8-575d-4e27-8127-83f99f5f781e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e87c2d-75e8-4fc3-9aae-6101dbb00f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataframe, label_encoder = None,feature_scaler = None, target_scaler = None, learn = False):\n",
    "    #Drop unused columns\n",
    "    data = dataframe.copy()\n",
    "    data.drop([\"init_tm\", \"met_var\", \"location\",  \"ens_var\", \"obs_tm\"], axis = 1, inplace = True)\n",
    "    data = data.to_numpy()\n",
    "    if learn == True:\n",
    "        label_encoder = LabelEncoder()\n",
    "        feature_scaler = StandardScaler()\n",
    "        target_scaler = StandardScaler()\n",
    "        #Learn label encoding for horizons\n",
    "        label = label_encoder.fit_transform(data[:,0])\n",
    "        #Learn target scaling\n",
    "        target_scaled = target_scaler.fit_transform(data[:,1].reshape(-1,1))\n",
    "        #Learn feature scaling\n",
    "        feature_scaled = feature_scaler.fit_transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data, label_encoder, feature_scaler, target_scaler\n",
    "    \n",
    "    else:\n",
    "        #Learn labels\n",
    "        label = label_encoder.transform(data[:,0])\n",
    "        #Scale target\n",
    "        target_scaled = target_scaler.transform(data[:,1].reshape(-1,1))\n",
    "        #Scale features\n",
    "        feature_scaled = feature_scaler.transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57784577-06ee-46fc-90fd-553536e0793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(input_data, predict = False):\n",
    "    #Extract forecast embedding\n",
    "    horizon_emb = input_data[:,0]\n",
    "    \n",
    "    if predict == False:        \n",
    "        #Extract features\n",
    "        features = input_data[:,2:]\n",
    "        # Extract target\n",
    "        target = np.expand_dims(input_data[:,1],1)\n",
    "        return [features, horizon_emb], target\n",
    "    else:\n",
    "        #Extract features\n",
    "        features = input_data[:,1:]\n",
    "        return [features, horizon_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3de855-a07a-44dd-b2b1-0415c0d27655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, train_target, validation_data, batch_size, epochs, learning_rate, fine_tuning = True):\n",
    "    model = base_model()    \n",
    "    #Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    #Callbacks\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "    model.compile(optimizer = optimizer, loss = lambda true,pred: pinball_loss(true, pred, tau = quantiles))\n",
    "    #model.compile(optimizer = optimizer, loss = lambda true,pred: smooth_pinball_loss(true, pred, tau = quantiles))\n",
    "    #Normal fit\n",
    "    history1 = model.fit(x = train_data, y = train_target, validation_data = validation_data, epochs = epochs, batch_size = batch_size, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    \n",
    "    #Fine tuning\n",
    "    if fine_tuning == True:\n",
    "        enc_horizons = label_encoder.transform(horizons)\n",
    "        train_filtering = np.isin(train_data[1], enc_horizons)\n",
    "        train_data_fine = [train_data[0][train_filtering], train_data[1][train_filtering]]\n",
    "        train_target_fine = train_target[train_filtering]\n",
    "        #Val filtering\n",
    "        val_data, val_target = validation_data\n",
    "        val_filtering = np.isin(val_data[1], enc_horizons)\n",
    "        val_data_fine = [val_data[0][val_filtering], val_data[1][val_filtering]]\n",
    "        val_target_fine = val_target[val_filtering]\n",
    "        validation_data_fine = (val_data_fine, val_target_fine)\n",
    "        \n",
    "        #New optimizer\n",
    "        history2 = model.fit(x = train_data_fine, y = train_target_fine, validation_data = validation_data_fine, epochs = epochs, batch_size = 256, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    return model, [history1, history2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c92120-2ab7-4e20-a369-f4a180604891",
   "metadata": {},
   "source": [
    "## Read data and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5f670-d216-4e78-b696-2248228501f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "horizons = [36, 48 ,60, 72, 84]\n",
    "n_encodings = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441417c7-2bec-4a66-9d44-c89a37801b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind data\n",
    "wind_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_wind_10m.feather\")\n",
    "#Pressure data\n",
    "pressure_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_mslp.feather\")\n",
    "pressure_data.rename({\"ens_mean\":\"mean_pressure\"}, axis = 1, inplace = True)\n",
    "#Cloud data\n",
    "cloud_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_clct.feather\")\n",
    "cloud_data.rename({\"ens_mean\":\"cloud_coverage\"}, axis = 1, inplace = True)\n",
    "#Vmax data\n",
    "max_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_vmax_10m.feather\")\n",
    "max_data.rename({\"ens_mean\":\"vmax\"}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "data = wind_data.merge(pressure_data[[\"init_tm\",\"fcst_hour\",\"mean_pressure\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(cloud_data[[\"init_tm\",\"fcst_hour\",\"cloud_coverage\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(max_data[[\"init_tm\",\"fcst_hour\",\"vmax\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "#Replace vmax NaNs by mean\n",
    "vmax_mean = data[\"vmax\"].mean()\n",
    "data.loc[:,\"vmax\"].fillna(vmax_mean, inplace = True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "#Positional encoding\n",
    "pos_enc = pd.DataFrame(index=pd.DatetimeIndex(data[\"obs_tm\"]))\n",
    "pos_enc[\"Dayofyear\"] = pos_enc.index.dayofyear\n",
    "pos_enc[\"n_days\"] = 365\n",
    "pos_enc.loc[pos_enc.index.year==2020,\"n_days\"] = 366\n",
    "#Calculate actual positional encoding\n",
    "cos_encoding = np.cos(2*math.pi*pos_enc[\"Dayofyear\"]/pos_enc[\"n_days\"])\n",
    "data[\"pos_enc_1\"] = cos_encoding.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd2367-25dd-444a-a111-c8b7abc224e8",
   "metadata": {},
   "source": [
    "## Create study object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab120e-a398-4eda-98b7-a07aff811d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_index, test_index, data):\n",
    "    #Get split\n",
    "    train_df = data.loc[train_index]\n",
    "    test_df = data.loc[test_index]\n",
    "\n",
    "    #Normalize data\n",
    "    train, label_encoder, feature_scaler, target_scaler = normalize(train_df, learn = True)\n",
    "    test = normalize(test_df, label_encoder, feature_scaler, target_scaler)\n",
    "    n_encodings = len(np.unique(train[:,0]))\n",
    "\n",
    "    #Convert format\n",
    "    train_data, train_target = convert_format(train)\n",
    "    test_data, test_target = convert_format(test)\n",
    "    \n",
    "    return train_data, train_target, test_data, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997f6ce-c20c-4fed-86b7-988b6891fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(trial, quantiles = quantiles):\n",
    "    #Sample alpha\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-5, 0.01)\n",
    "    loss = trial.suggest_categorical(\"loss\", [\"pinball\",\"exp\",\"abs\",\"huber\"])\n",
    "    losses = {\n",
    "    \"pinball\": lambda true,pred: pinball_loss(true, pred, tau = quantiles),\n",
    "    \"exp\": lambda true,pred: exp_pinball_loss(true, pred, tau = quantiles, alpha = alpha),\n",
    "    \"abs\": lambda true,pred: sqrt_pinball_loss(true, pred, tau = quantiles, alpha = alpha),\n",
    "    \"huber\": lambda true,pred: huber_pinball_loss(true, pred, tau = quantiles, alpha = alpha)}\n",
    "\n",
    "\n",
    "    loss_func = losses[loss]\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f381d58c-7ac9-4331-a9c7-1c4157daf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(trial):\n",
    "    # Copied from optuna tutorial\n",
    "    kwargs = {}\n",
    "    optimizer_options = [\"Adam\", \"SGD\"]\n",
    "    optimizer_selected = trial.suggest_categorical(\"optimizer\", optimizer_options)\n",
    "    if optimizer_selected == \"Adam\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    elif optimizer_selected == \"SGD\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\n",
    "            \"sgd_opt_learning_rate\", 1e-5, 1e-2, log=True\n",
    "        )\n",
    "    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06273e-d926-431b-b55b-c500d4c2c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    #Parameters\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.05, 0.5, log = True)\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 2, 6, step = 2)\n",
    "    n_layers = trial.suggest_int(\"n_layers\",1,2,1)\n",
    "    n_units_1 = trial.suggest_int(\"n_units_1\",16,128, log =True)\n",
    "    if n_layers == 2:\n",
    "        n_units_2 = trial.suggest_int(\"n_units_2\", 16, 128, log = True)\n",
    "    else:\n",
    "        n_units_2 = 1\n",
    "    #Create Model\n",
    "    class base_model(tf.keras.Model):    \n",
    "        def __init__(self, n_layers, n_units_1, n_units_2, embedding_dim, dropout_rate, n_embeddings = n_encodings):\n",
    "            super(base_model, self).__init__()\n",
    "            #Embedding layers\n",
    "            self.embedding = Embedding(input_dim = n_embeddings, output_dim = embedding_dim)\n",
    "            #N_layers\n",
    "            self.n_layers = n_layers\n",
    "            #Dropout\n",
    "            self.dropout = Dropout(dropout_rate)\n",
    "            #Create Dense layers\n",
    "            self.hidden = Dense(n_units_1, activation = \"relu\")\n",
    "            self.hidden2 = Dense(n_units_2, activation = \"relu\")\n",
    "            self.out = Dense(5, activation = \"linear\")\n",
    "\n",
    "        def call(self, input_data):\n",
    "            #Extract data\n",
    "            features, horizon_emb = input_data\n",
    "            #Calculate embedding\n",
    "            emb = self.embedding(horizon_emb)\n",
    "            emb = tf.squeeze(emb, axis = 1)\n",
    "            conc = Concatenate(axis = 1)([features, emb])\n",
    "            #Calculate output\n",
    "            output = self.hidden(conc)\n",
    "            if self.n_layers == 2:\n",
    "                output = self.hidden2(output)\n",
    "            output = self.dropout(output)\n",
    "            output = self.out(output)\n",
    "            return output\n",
    "\n",
    "    #Train\n",
    "    model = base_model(n_layers, n_units_1, n_units_2, embedding_dim, dropout_rate)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a485b-9855-47bb-ab96-107e0803aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, loss_func, optimizer, batch_size, epochs = 100,  data = data, n_splits = 10):\n",
    "    #Get data\n",
    "    data = data.reset_index().drop(\"index\",axis=1)\n",
    "    fold = KFold(n_splits = n_splits, shuffle = True, random_state = 10)\n",
    "    split = fold.split(data.index)\n",
    "    \n",
    "    #Total loss\n",
    "    test_loss = 0\n",
    "    for train_index, test_index in split:\n",
    "        #Get data\n",
    "        train_data, train_target, test_data, test_target = get_data(train_index, test_index, data)\n",
    "\n",
    "        #Compile model\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "        model.compile(optimizer = optimizer, loss = loss_func)\n",
    "        \n",
    "        #Normal fit\n",
    "        history1 = model.fit(x = train_data, y = train_target, validation_split = 0.2, epochs = epochs, batch_size = batch_size, callbacks = [callback], shuffle = True, verbose = False)\n",
    "\n",
    "        #Calculate loss\n",
    "        pred = model.predict(test_data)\n",
    "        total_loss = 0\n",
    "        for cnt,quantile in enumerate(quantiles):\n",
    "            loss = mean_pinball_loss(test_target.reshape(-1), pred[:,cnt].reshape(-1), alpha = quantile)\n",
    "            total_loss += loss\n",
    "\n",
    "        test_loss += total_loss/len(quantiles)\n",
    "\n",
    "    return test_loss/n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a288a49-3b39-406f-9139-0498184a769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Create Parameters\n",
    "    batch_size = 2**trial.suggest_int(\"batch_size\",7,10,1)\n",
    "    epochs = 100\n",
    "    optimizer = get_optimizer(trial)    \n",
    "    #Get loss\n",
    "    loss_func = get_loss(trial)        \n",
    "    #Get Model\n",
    "    model = create_model(trial)    \n",
    "    #Train model\n",
    "    loss = learn(model = model, loss_func = loss_func, optimizer = optimizer, batch_size = batch_size, epochs = epochs, n_splits = 10)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abad412-75b4-45db-95e8-860055b5fd2d",
   "metadata": {},
   "source": [
    "## Run study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4da80-a95c-43f4-acf0-6569c52564d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_study = opt.create_study(direction = 'minimize')\n",
    "wind_study.optimize(objective, n_trials = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd0668-f29f-4d73-acaa-60b9ca7e83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wind_study.best_value)\n",
    "wind_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4d824-cd5c-4951-a81c-eadb3614b929",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tune temperature model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07825ebd-1bce-418f-a954-ea230a554fc7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b17bb-1b86-4351-9da8-b87f005a2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_t_2m.feather\")\n",
    "data.dropna(inplace=True)\n",
    "#Positional encoding\n",
    "pos_enc = pd.DataFrame(index=pd.DatetimeIndex(data[\"obs_tm\"]))\n",
    "pos_enc[\"Dayofyear\"] = pos_enc.index.dayofyear\n",
    "pos_enc[\"n_days\"] = 365\n",
    "pos_enc.loc[pos_enc.index.year==2020,\"n_days\"] = 366\n",
    "#Calculate actual positional encoding\n",
    "cos_encoding = np.cos(2*math.pi*pos_enc[\"Dayofyear\"]/pos_enc[\"n_days\"])\n",
    "data[\"pos_enc_1\"] = cos_encoding.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7b381-03b4-4663-a35e-18d5fdf0d243",
   "metadata": {},
   "source": [
    "## Run study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17ac63-2614-4810-b9a6-461ea97a3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_study = opt.create_study(direction = 'minimize')\n",
    "temp_study.optimize(objective, n_trials = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24ec4e-8616-4ad0-8786-ac9dedcfaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_study.best_value)\n",
    "temp_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40441c18-946f-4e1e-9c6d-6a142716b042",
   "metadata": {},
   "source": [
    "# Compare Loss functions\n",
    "\n",
    "Compare Loss functions by tuning only loss function for wind model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b494cd2-6b7d-446f-ac92-d49ae1fbcd31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31067f-ca12-45a9-83a8-69e175aad714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataframe, label_encoder = None,feature_scaler = None, target_scaler = None, learn = False):\n",
    "    #Drop unused columns\n",
    "    data = dataframe.copy()\n",
    "    data.drop([\"init_tm\", \"met_var\", \"location\",  \"ens_var\", \"obs_tm\"], axis = 1, inplace = True)\n",
    "    data = data.to_numpy()\n",
    "    if learn == True:\n",
    "        label_encoder = LabelEncoder()\n",
    "        feature_scaler = StandardScaler()\n",
    "        target_scaler = StandardScaler()\n",
    "        #Learn label encoding for horizons\n",
    "        label = label_encoder.fit_transform(data[:,0])\n",
    "        #Learn target scaling\n",
    "        target_scaled = target_scaler.fit_transform(data[:,1].reshape(-1,1))\n",
    "        #Learn feature scaling\n",
    "        feature_scaled = feature_scaler.fit_transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data, label_encoder, feature_scaler, target_scaler\n",
    "    \n",
    "    else:\n",
    "        #Learn labels\n",
    "        label = label_encoder.transform(data[:,0])\n",
    "        #Scale target\n",
    "        target_scaled = target_scaler.transform(data[:,1].reshape(-1,1))\n",
    "        #Scale features\n",
    "        feature_scaled = feature_scaler.transform(data[:,2:])\n",
    "        #Append\n",
    "        data[:,0] = label\n",
    "        data[:,1] = target_scaled.reshape(-1)\n",
    "        data[:,2:] = feature_scaled\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d34a5-5ca9-4a3e-9ffc-d55c977731c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(input_data, predict = False):\n",
    "    #Extract forecast embedding\n",
    "    horizon_emb = input_data[:,0]\n",
    "    \n",
    "    if predict == False:        \n",
    "        #Extract features\n",
    "        features = input_data[:,2:]\n",
    "        # Extract target\n",
    "        target = np.expand_dims(input_data[:,1],1)\n",
    "        return [features, horizon_emb], target\n",
    "    else:\n",
    "        #Extract features\n",
    "        features = input_data[:,1:]\n",
    "        return [features, horizon_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1a2a9-2803-4434-9f74-99878c3f6c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, train_target, validation_data, batch_size, epochs, learning_rate, fine_tuning = True):\n",
    "    model = base_model()    \n",
    "    #Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    #Callbacks\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "    model.compile(optimizer = optimizer, loss = lambda true,pred: pinball_loss(true, pred, tau = quantiles))\n",
    "    #model.compile(optimizer = optimizer, loss = lambda true,pred: smooth_pinball_loss(true, pred, tau = quantiles))\n",
    "    #Normal fit\n",
    "    history1 = model.fit(x = train_data, y = train_target, validation_data = validation_data, epochs = epochs, batch_size = batch_size, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    \n",
    "    #Fine tuning\n",
    "    if fine_tuning == True:\n",
    "        enc_horizons = label_encoder.transform(horizons)\n",
    "        train_filtering = np.isin(train_data[1], enc_horizons)\n",
    "        train_data_fine = [train_data[0][train_filtering], train_data[1][train_filtering]]\n",
    "        train_target_fine = train_target[train_filtering]\n",
    "        #Val filtering\n",
    "        val_data, val_target = validation_data\n",
    "        val_filtering = np.isin(val_data[1], enc_horizons)\n",
    "        val_data_fine = [val_data[0][val_filtering], val_data[1][val_filtering]]\n",
    "        val_target_fine = val_target[val_filtering]\n",
    "        validation_data_fine = (val_data_fine, val_target_fine)\n",
    "        \n",
    "        #New optimizer\n",
    "        history2 = model.fit(x = train_data_fine, y = train_target_fine, validation_data = validation_data_fine, epochs = epochs, batch_size = 256, callbacks = [callback], shuffle = True, verbose = False)\n",
    "    return model, [history1, history2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5db94a-5974-4fbb-9419-35bf625dbb99",
   "metadata": {},
   "source": [
    "## Read data and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc3f41-60c1-4eaa-9368-6de794120821",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "horizons = [36, 48 ,60, 72, 84]\n",
    "n_encodings = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84630e3f-5acb-4a53-b430-620779167185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind data\n",
    "wind_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_wind_10m.feather\")\n",
    "#Pressure data\n",
    "pressure_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_mslp.feather\")\n",
    "pressure_data.rename({\"ens_mean\":\"mean_pressure\"}, axis = 1, inplace = True)\n",
    "#Cloud data\n",
    "cloud_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_clct.feather\")\n",
    "cloud_data.rename({\"ens_mean\":\"cloud_coverage\"}, axis = 1, inplace = True)\n",
    "#Vmax data\n",
    "max_data = pd.read_feather(\"data/berlin_data/historic_data/icon_eps_vmax_10m.feather\")\n",
    "max_data.rename({\"ens_mean\":\"vmax\"}, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "data = wind_data.merge(pressure_data[[\"init_tm\",\"fcst_hour\",\"mean_pressure\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(cloud_data[[\"init_tm\",\"fcst_hour\",\"cloud_coverage\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "data = data.merge(max_data[[\"init_tm\",\"fcst_hour\",\"vmax\"]], on = [\"init_tm\",\"fcst_hour\"], how = \"left\")\n",
    "#Replace vmax NaNs by mean\n",
    "vmax_mean = data[\"vmax\"].mean()\n",
    "data.loc[:,\"vmax\"].fillna(vmax_mean, inplace = True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "#Positional encoding\n",
    "pos_enc = pd.DataFrame(index=pd.DatetimeIndex(data[\"obs_tm\"]))\n",
    "pos_enc[\"Dayofyear\"] = pos_enc.index.dayofyear\n",
    "pos_enc[\"n_days\"] = 365\n",
    "pos_enc.loc[pos_enc.index.year==2020,\"n_days\"] = 366\n",
    "#Calculate actual positional encoding\n",
    "cos_encoding = np.cos(2*math.pi*pos_enc[\"Dayofyear\"]/pos_enc[\"n_days\"])\n",
    "data[\"pos_enc_1\"] = cos_encoding.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4952b5-14fa-4dac-b706-a9525cf6c74b",
   "metadata": {},
   "source": [
    "## Create study object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea16ca0-9d42-4488-af4f-59ada45cc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_index, test_index, data):\n",
    "    #Get split\n",
    "    train_df = data.loc[train_index]\n",
    "    test_df = data.loc[test_index]\n",
    "\n",
    "    #Normalize data\n",
    "    train, label_encoder, feature_scaler, target_scaler = normalize(train_df, learn = True)\n",
    "    test = normalize(test_df, label_encoder, feature_scaler, target_scaler)\n",
    "    n_encodings = len(np.unique(train[:,0]))\n",
    "\n",
    "    #Convert format\n",
    "    train_data, train_target = convert_format(train)\n",
    "    test_data, test_target = convert_format(test)\n",
    "    \n",
    "    return train_data, train_target, test_data, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c193b09-c0fb-423f-a905-e5e8d315c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss, alpha, quantiles = quantiles):\n",
    "    #Sample alpha\n",
    "    losses = {\n",
    "    \"pinball\": lambda true,pred: pinball_loss(true, pred, tau = quantiles),\n",
    "    \"exp\": lambda true,pred: exp_pinball_loss(true, pred, tau = quantiles, alpha = alpha),\n",
    "    \"abs\": lambda true,pred: sqrt_pinball_loss(true, pred, tau = quantiles, alpha = alpha),\n",
    "    \"huber\": lambda true,pred: huber_pinball_loss(true, pred, tau = quantiles, alpha = alpha)\n",
    "    }\n",
    "    loss_func = losses[loss]\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dfaba-4146-467b-8a2b-d53f8caa33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    #Parameters\n",
    "    dropout_rate = 0.1\n",
    "    n_layers = 2\n",
    "    n_units_1 = 58\n",
    "    if n_layers == 2:\n",
    "        n_units_2 = 64\n",
    "    else:\n",
    "        n_units_2 = 1\n",
    "    #Create Model\n",
    "    class base_model(tf.keras.Model):    \n",
    "        def __init__(self, n_layers, n_units_1, n_units_2, dropout_rate, n_embeddings = n_encodings):\n",
    "            super(base_model, self).__init__()\n",
    "            #Embedding layers\n",
    "            self.embedding = Embedding(input_dim = n_embeddings, output_dim = 4)\n",
    "            #N_layers\n",
    "            self.n_layers = n_layers\n",
    "            #Dropout\n",
    "            self.dropout = Dropout(dropout_rate)\n",
    "            #Create Dense layers\n",
    "            self.hidden = Dense(n_units_1, activation = \"relu\")\n",
    "            self.hidden2 = Dense(n_units_2, activation = \"relu\")\n",
    "            self.out = Dense(5, activation = \"linear\")\n",
    "\n",
    "        def call(self, input_data):\n",
    "            #Extract data\n",
    "            features, horizon_emb = input_data\n",
    "            #Calculate embedding\n",
    "            emb = self.embedding(horizon_emb)\n",
    "            emb = tf.squeeze(emb, axis = 1)\n",
    "            conc = Concatenate(axis = 1)([features, emb])\n",
    "            #Calculate output\n",
    "            output = self.hidden(conc)\n",
    "            if self.n_layers == 2:\n",
    "                output = self.hidden2(output)\n",
    "            output = self.dropout(output)\n",
    "            output = self.out(output)\n",
    "            return output\n",
    "\n",
    "    #Train\n",
    "    model = base_model(n_layers, n_units_1, n_units_2, dropout_rate)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0959e-b081-4537-9e06-8f158ca3df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, loss_func, optimizer, batch_size, epochs = 100,  data = data, n_splits = 10):\n",
    "    #Get data\n",
    "    data = data.reset_index().drop(\"index\",axis=1)\n",
    "    fold = KFold(n_splits = n_splits, shuffle = True, random_state = 10)\n",
    "    split = fold.split(data.index)\n",
    "    \n",
    "    #Total loss\n",
    "    test_loss = 0\n",
    "    for train_index, test_index in split:\n",
    "        #Get data\n",
    "        train_data, train_target, test_data, test_target = get_data(train_index, test_index, data)\n",
    "\n",
    "        #Compile model\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 7, min_delta = 1e-5)\n",
    "        model.compile(optimizer = optimizer, loss = loss_func)\n",
    "        \n",
    "        #Normal fit\n",
    "        history1 = model.fit(x = train_data, y = train_target, validation_split = 0.2, epochs = epochs, batch_size = batch_size, callbacks = [callback], shuffle = True, verbose = False)\n",
    "\n",
    "        #Calculate loss\n",
    "        pred = model.predict(test_data)\n",
    "        total_loss = 0\n",
    "        for cnt,quantile in enumerate(quantiles):\n",
    "            loss = mean_pinball_loss(test_target.reshape(-1), pred[:,cnt].reshape(-1), alpha = quantile)\n",
    "            total_loss += loss\n",
    "\n",
    "        test_loss += total_loss/len(quantiles)\n",
    "\n",
    "    return test_loss/n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b518ba8-596a-4c79-8b09-0598b11258f2",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa2aad-91ec-49ba-836f-ed51ca75cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters\n",
    "alpha_space = np.logspace(start = np.log10(1e-5), stop = np.log10(1), num = 30)\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d33f0-7d4e-42e0-b898-87c419bd92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataframe\n",
    "results = pd.DataFrame(index = np.arange(120), columns = [\"Method\", \"alpha\", \"Pinball loss\"])\n",
    "\n",
    "for cnt_loss, loss in enumerate([\"pinball\",\"exp\",\"abs\",\"huber\"]):\n",
    "    for cnt_alpha,alpha in enumerate(alpha_space):\n",
    "        loss_func = get_loss(loss,alpha)    \n",
    "        model = create_model() \n",
    "        pinball_loss = learn(model = model, loss_func = loss_func, optimizer = optimizer, batch_size = batch_size, epochs = epochs, n_splits = 5)\n",
    "        results.loc[cnt_loss*30+cnt_alpha] = [loss, alpha, pinball_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5cce2-dc31-4b6e-ab77-dd1aed709ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle(\"loss_experiment_results.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
